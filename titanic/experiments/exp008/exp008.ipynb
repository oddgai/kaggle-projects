{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp008 - Advanced Feature Engineering (Kaggleãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å‚è€ƒ)\n",
    "\n",
    "## ğŸ¯ ç›®æ¨™\n",
    "- é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•ã®å®Ÿè£…\n",
    "- Titleã€Cabinã€Ticketã€Familyã®è©³ç´°åˆ†æ\n",
    "- exp004ï¼ˆ0.77990ï¼‰ã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç›®æŒ‡ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import re\n",
    "\n",
    "plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "\n",
    "print(\"ğŸš€ exp008 - Advanced Feature Engineering\")\n",
    "print(\"Kaggleãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®æ‰‹æ³•ã‚’å‚è€ƒã«å®Ÿè£…\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "train_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/train.csv')\n",
    "test_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/test.csv')\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿çµåˆï¼ˆå‰å‡¦ç†ã®ä¸€è²«æ€§ã®ãŸã‚ï¼‰\n",
    "df_all = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n",
    "df_all['is_train'] = df_all['Survived'].notna()\n",
    "\n",
    "print(f\"\\nå…¨ãƒ‡ãƒ¼ã‚¿ shape: {df_all.shape}\")\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {df_all['is_train'].sum()}ä»¶\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {(~df_all['is_train']).sum()}ä»¶\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Phase 1: Titleï¼ˆç§°å·ï¼‰ã®é«˜åº¦ãªå‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Titleã®æŠ½å‡ºã¨è©³ç´°åˆ†æ\n",
    "df_all['Title'] = df_all['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "print(\"=== Titleåˆ†å¸ƒ ===\")\n",
    "print(df_all['Title'].value_counts())\n",
    "\n",
    "# Titleåˆ¥ã®ç”Ÿå­˜ç‡ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰\n",
    "title_survival = df_all[df_all['is_train']].groupby('Title')['Survived'].agg(['mean', 'count']).round(3)\n",
    "print(\"\\n=== Titleåˆ¥ç”Ÿå­˜ç‡ ===\")\n",
    "print(title_survival.sort_values('mean', ascending=False))\n",
    "\n",
    "# Titleã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†é¡ï¼‰\n",
    "title_mapping = {\n",
    "    # ä¸€èˆ¬çš„ãªç§°å·\n",
    "    'Mr': 'Mr',\n",
    "    'Miss': 'Miss',\n",
    "    'Mrs': 'Mrs',\n",
    "    'Master': 'Master',\n",
    "    \n",
    "    # ãƒ•ãƒ©ãƒ³ã‚¹èªã®ç§°å·\n",
    "    'Mlle': 'Miss',  # Mademoiselle\n",
    "    'Mme': 'Mrs',    # Madame\n",
    "    'Ms': 'Miss',\n",
    "    \n",
    "    # è»äºº\n",
    "    'Col': 'Officer',\n",
    "    'Major': 'Officer',\n",
    "    'Capt': 'Officer',\n",
    "    \n",
    "    # è²´æ—\n",
    "    'Lady': 'Royalty',\n",
    "    'Sir': 'Royalty',\n",
    "    'Countess': 'Royalty',\n",
    "    'Don': 'Royalty',\n",
    "    'Dona': 'Royalty',\n",
    "    'Jonkheer': 'Royalty',\n",
    "    \n",
    "    # è–è·è€…ãƒ»å°‚é–€è·\n",
    "    'Dr': 'Dr',\n",
    "    'Rev': 'Rev'\n",
    "}\n",
    "\n",
    "df_all['Title_Grouped'] = df_all['Title'].map(title_mapping)\n",
    "\n",
    "# å¹´é½¢ã¨Titleã®çµ„ã¿åˆã‚ã›ã§è©³ç´°åŒ–\n",
    "df_all['Is_Child'] = ((df_all['Title'] == 'Master') | \n",
    "                      ((df_all['Title'] == 'Miss') & (df_all['Age'] < 18))).astype(int)\n",
    "\n",
    "df_all['Is_Young_Miss'] = ((df_all['Title'] == 'Miss') & \n",
    "                           (df_all['Age'] >= 18) & (df_all['Age'] < 30)).astype(int)\n",
    "\n",
    "df_all['Is_Mrs'] = (df_all['Title'] == 'Mrs').astype(int)\n",
    "\n",
    "print(\"\\n=== Titleç‰¹å¾´é‡ä½œæˆå®Œäº† ===\")\n",
    "print(f\"Title_Groupedåˆ†å¸ƒ:\")\n",
    "print(df_all['Title_Grouped'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ  Phase 2: Cabinï¼ˆå®¢å®¤ï¼‰ã®è©³ç´°å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cabinæƒ…å ±ã®è©³ç´°æŠ½å‡º\n",
    "df_all['Has_Cabin'] = df_all['Cabin'].notna().astype(int)\n",
    "\n",
    "# ãƒ‡ãƒƒã‚­ã®æŠ½å‡º\n",
    "df_all['Deck'] = df_all['Cabin'].str[0]\n",
    "\n",
    "# è¤‡æ•°å®¢å®¤ã‚’æŒã¤ä¹—å®¢ã®å‡¦ç†\n",
    "df_all['Cabin_Count'] = df_all['Cabin'].str.split().str.len()\n",
    "df_all['Cabin_Count'] = df_all['Cabin_Count'].fillna(0)\n",
    "\n",
    "# ãƒ‡ãƒƒã‚­ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆæ•‘å‘½ãƒœãƒ¼ãƒˆã¸ã®è¿‘ã•ï¼‰\n",
    "deck_mapping = {\n",
    "    'A': 7, 'B': 6, 'C': 5, 'D': 4,\n",
    "    'E': 3, 'F': 2, 'G': 1, 'T': 0\n",
    "}\n",
    "df_all['Deck_Num'] = df_all['Deck'].map(deck_mapping)\n",
    "\n",
    "# Pclassã¨ãƒ‡ãƒƒã‚­ã®ç›¸é–¢ã‚’åˆ©ç”¨ã—ãŸæ¬ æå€¤è£œå®Œ\n",
    "for pclass in [1, 2, 3]:\n",
    "    deck_mode = df_all[(df_all['Pclass'] == pclass) & df_all['Deck'].notna()]['Deck'].mode()\n",
    "    if len(deck_mode) > 0:\n",
    "        df_all.loc[(df_all['Pclass'] == pclass) & df_all['Deck'].isna(), 'Deck'] = deck_mode[0]\n",
    "\n",
    "# ãƒ‡ãƒƒã‚­ãŒä¸æ˜ãªå ´åˆã¯Pclassãƒ™ãƒ¼ã‚¹ã§æ¨å®š\n",
    "df_all['Deck'] = df_all['Deck'].fillna('U')  # Unknown\n",
    "df_all['Deck_Num'] = df_all['Deck_Num'].fillna(-1)\n",
    "\n",
    "# å®¢å®¤ç•ªå·ã‹ã‚‰ä½ç½®ã‚’æ¨å®šï¼ˆæ•°å€¤éƒ¨åˆ†ï¼‰\n",
    "def extract_cabin_number(cabin):\n",
    "    if pd.isna(cabin):\n",
    "        return -1\n",
    "    numbers = re.findall(r'\\d+', str(cabin))\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    return -1\n",
    "\n",
    "df_all['Cabin_Number'] = df_all['Cabin'].apply(extract_cabin_number)\n",
    "\n",
    "# å®¢å®¤ä½ç½®ã‚¹ã‚³ã‚¢ï¼ˆå‰æ–¹/ä¸­å¤®/å¾Œæ–¹ï¼‰\n",
    "df_all['Cabin_Position'] = pd.cut(df_all['Cabin_Number'], \n",
    "                                   bins=[-2, 0, 50, 100, 200],\n",
    "                                   labels=['Unknown', 'Front', 'Middle', 'Back'])\n",
    "\n",
    "print(\"=== Cabinç‰¹å¾´é‡ä½œæˆå®Œäº† ===\")\n",
    "print(f\"\\nãƒ‡ãƒƒã‚­åˆ†å¸ƒ:\")\n",
    "print(df_all['Deck'].value_counts())\n",
    "print(f\"\\nè¤‡æ•°å®¢å®¤ä¿æœ‰è€…: {(df_all['Cabin_Count'] > 1).sum()}äºº\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ« Phase 3: Ticketï¼ˆãƒã‚±ãƒƒãƒˆï¼‰ã®è©³ç´°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ãƒã‚±ãƒƒãƒˆç•ªå·ã®å‰å‡¦ç†\n",
    "df_all['Ticket_Cleaned'] = df_all['Ticket'].str.replace('[^A-Za-z0-9]', '', regex=True)\n",
    "\n",
    "# ãƒã‚±ãƒƒãƒˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®æŠ½å‡º\n",
    "df_all['Ticket_Prefix'] = df_all['Ticket'].str.extract(r'([A-Za-z\\.]+)', expand=False)\n",
    "df_all['Ticket_Prefix'] = df_all['Ticket_Prefix'].str.replace('[^A-Za-z]', '', regex=True)\n",
    "df_all['Ticket_Prefix'] = df_all['Ticket_Prefix'].fillna('NONE')\n",
    "\n",
    "# ãƒã‚±ãƒƒãƒˆç•ªå·ï¼ˆæ•°å€¤éƒ¨åˆ†ï¼‰ã®æŠ½å‡º\n",
    "df_all['Ticket_Number'] = df_all['Ticket'].str.extract(r'(\\d+)', expand=False)\n",
    "df_all['Ticket_Number'] = pd.to_numeric(df_all['Ticket_Number'], errors='coerce')\n",
    "\n",
    "# ãƒã‚±ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºï¼ˆåŒã˜ãƒã‚±ãƒƒãƒˆç•ªå·ã‚’æŒã¤äººæ•°ï¼‰\n",
    "ticket_counts = df_all['Ticket'].value_counts()\n",
    "df_all['Ticket_Group_Size'] = df_all['Ticket'].map(ticket_counts)\n",
    "\n",
    "# é€£ç•ªãƒã‚±ãƒƒãƒˆã®è­˜åˆ¥ï¼ˆå®¶æ—ã‚„å›£ä½“ã®å¯èƒ½æ€§ï¼‰\n",
    "df_all['Ticket_Number_Sorted'] = df_all.groupby('Ticket_Prefix')['Ticket_Number'].rank(method='dense')\n",
    "\n",
    "# ãƒã‚±ãƒƒãƒˆä¾¡æ ¼ã®æ­£è¦åŒ–ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã§å‰²ã‚‹ï¼‰\n",
    "df_all['Fare_Per_Person'] = df_all['Fare'] / df_all['Ticket_Group_Size']\n",
    "\n",
    "# ãƒã‚±ãƒƒãƒˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹åˆ¥ã®çµ±è¨ˆ\n",
    "prefix_stats = df_all[df_all['is_train']].groupby('Ticket_Prefix')['Survived'].agg(['mean', 'count'])\n",
    "prefix_stats = prefix_stats[prefix_stats['count'] >= 5]  # 5ä»¶ä»¥ä¸Šã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ã¿\n",
    "\n",
    "print(\"=== ä¸»è¦ãƒã‚±ãƒƒãƒˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹åˆ¥ç”Ÿå­˜ç‡ ===\")\n",
    "print(prefix_stats.sort_values('mean', ascending=False).head(10))\n",
    "\n",
    "# å¸Œå°‘ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®çµ±åˆ\n",
    "frequent_prefixes = prefix_stats[prefix_stats['count'] >= 10].index.tolist()\n",
    "df_all['Ticket_Prefix_Grouped'] = df_all['Ticket_Prefix'].apply(\n",
    "    lambda x: x if x in frequent_prefixes else 'OTHER'\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Ticketç‰¹å¾´é‡ä½œæˆå®Œäº† ===\")\n",
    "print(f\"ã‚°ãƒ«ãƒ¼ãƒ—ãƒã‚±ãƒƒãƒˆ: {(df_all['Ticket_Group_Size'] > 1).sum()}ä»¶\")\n",
    "print(f\"ä¸€äººã‚ãŸã‚Šé‹è³ƒä¸­å¤®å€¤: ${df_all['Fare_Per_Person'].median():.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Phase 4: Familyï¼ˆå®¶æ—ï¼‰ã®è¤‡é›‘ãªé–¢ä¿‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# åŸºæœ¬çš„ãªå®¶æ—ã‚µã‚¤ã‚º\n",
    "df_all['FamilySize'] = df_all['SibSp'] + df_all['Parch'] + 1\n",
    "df_all['IsAlone'] = (df_all['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# å®¶æ—ã‚µã‚¤ã‚ºã®ã‚«ãƒ†ã‚´ãƒªåŒ–\n",
    "df_all['FamilySize_Cat'] = pd.cut(df_all['FamilySize'], \n",
    "                                   bins=[0, 1, 3, 5, 20],\n",
    "                                   labels=['Alone', 'Small', 'Medium', 'Large'])\n",
    "\n",
    "# è‹—å­—ã®æŠ½å‡º\n",
    "df_all['Surname'] = df_all['Name'].str.split(',').str[0]\n",
    "\n",
    "# åŒä¸€è‹—å­—ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚µã‚¤ã‚º\n",
    "surname_counts = df_all['Surname'].value_counts()\n",
    "df_all['Surname_Count'] = df_all['Surname'].map(surname_counts)\n",
    "\n",
    "# å¥³æ€§ã¨å­ä¾›ã‚’æŒã¤ç”·æ€§ã®è­˜åˆ¥ï¼ˆå®¶æ—ã®ä¿è­·è€…ï¼‰\n",
    "df_all['Is_Mother'] = ((df_all['Sex'] == 'female') & \n",
    "                       (df_all['Parch'] > 0) & \n",
    "                       (df_all['Age'] > 18)).astype(int)\n",
    "\n",
    "df_all['Is_Father'] = ((df_all['Sex'] == 'male') & \n",
    "                       (df_all['Parch'] > 0) & \n",
    "                       (df_all['Age'] > 18)).astype(int)\n",
    "\n",
    "# å®¶æ—ã‚¿ã‚¤ãƒ—ã®è©³ç´°åˆ†é¡\n",
    "def classify_family_type(row):\n",
    "    if row['IsAlone']:\n",
    "        return 'Alone'\n",
    "    elif row['Is_Mother']:\n",
    "        return 'Mother'\n",
    "    elif row['Is_Father']:\n",
    "        return 'Father'\n",
    "    elif row['Is_Child']:\n",
    "        return 'Child'\n",
    "    elif row['SibSp'] > 0 and row['Parch'] == 0:\n",
    "        return 'Sibling'\n",
    "    elif row['SibSp'] == 0 and row['Parch'] > 0:\n",
    "        return 'Parent_Child'\n",
    "    else:\n",
    "        return 'Extended'\n",
    "\n",
    "df_all['Family_Type'] = df_all.apply(classify_family_type, axis=1)\n",
    "\n",
    "# åŒä¸€è‹—å­—ãƒ»ãƒã‚±ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ç”Ÿå­˜ç‡ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—ï¼‰\n",
    "train_data = df_all[df_all['is_train']].copy()\n",
    "\n",
    "# è‹—å­—åˆ¥ç”Ÿå­˜ç‡\n",
    "surname_survival = train_data.groupby('Surname')['Survived'].agg(['mean', 'count'])\n",
    "surname_survival.columns = ['Surname_Survival_Rate', 'Surname_Group_Count']\n",
    "df_all = df_all.merge(surname_survival, on='Surname', how='left')\n",
    "\n",
    "# å°‘äººæ•°è‹—å­—ã®ç”Ÿå­˜ç‡ã¯å…¨ä½“å¹³å‡ã§è£œå®Œ\n",
    "overall_survival_rate = train_data['Survived'].mean()\n",
    "df_all.loc[df_all['Surname_Group_Count'] < 3, 'Surname_Survival_Rate'] = overall_survival_rate\n",
    "\n",
    "print(\"=== Family Typeåˆ†å¸ƒ ===\")\n",
    "print(df_all['Family_Type'].value_counts())\n",
    "\n",
    "# Family Typeåˆ¥ç”Ÿå­˜ç‡ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "family_survival = train_data.groupby('Family_Type')['Survived'].mean().round(3)\n",
    "print(\"\\n=== Family Typeåˆ¥ç”Ÿå­˜ç‡ ===\")\n",
    "print(family_survival.sort_values(ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Phase 5: æ¬ æå€¤ã®é«˜åº¦ãªè£œå®Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ageï¼ˆå¹´é½¢ï¼‰ã®æ¬ æå€¤è£œå®Œ\n",
    "# Titleã€Pclassã€Sexã€Fareã‚’ä½¿ã£ãŸäºˆæ¸¬\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# å¹´é½¢äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡\n",
    "age_features = ['Pclass', 'SibSp', 'Parch', 'Fare_Per_Person']\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå¹´é½¢äºˆæ¸¬ç”¨ï¼‰\n",
    "le_sex = LabelEncoder()\n",
    "df_all['Sex_Encoded'] = le_sex.fit_transform(df_all['Sex'])\n",
    "age_features.append('Sex_Encoded')\n",
    "\n",
    "le_title = LabelEncoder()\n",
    "df_all['Title_Encoded'] = le_title.fit_transform(df_all['Title_Grouped'].fillna('Unknown'))\n",
    "age_features.append('Title_Encoded')\n",
    "\n",
    "# å¹´é½¢äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«\n",
    "age_train = df_all[df_all['Age'].notna()][age_features + ['Age']].copy()\n",
    "age_test = df_all[df_all['Age'].isna()][age_features].copy()\n",
    "\n",
    "# æ¬ æå€¤å‡¦ç†\n",
    "age_train = age_train.fillna(age_train.median())\n",
    "age_test = age_test.fillna(age_train.median())\n",
    "\n",
    "if len(age_test) > 0:\n",
    "    rf_age = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_age.fit(age_train[age_features], age_train['Age'])\n",
    "    predicted_ages = rf_age.predict(age_test[age_features])\n",
    "    df_all.loc[df_all['Age'].isna(), 'Age'] = predicted_ages\n",
    "    print(f\"å¹´é½¢ã‚’{len(age_test)}ä»¶äºˆæ¸¬è£œå®Œ\")\n",
    "\n",
    "# Fareï¼ˆé‹è³ƒï¼‰ã®æ¬ æå€¤è£œå®Œ\n",
    "# Pclassã€Embarkedã€Ticket_Prefixãƒ™ãƒ¼ã‚¹ã§ä¸­å¤®å€¤è£œå®Œ\n",
    "df_all['Fare'] = df_all.groupby(['Pclass', 'Embarked'])['Fare'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "df_all['Fare'] = df_all['Fare'].fillna(df_all['Fare'].median())\n",
    "df_all['Fare_Per_Person'] = df_all['Fare_Per_Person'].fillna(df_all['Fare_Per_Person'].median())\n",
    "\n",
    "# Embarkedï¼ˆä¹—èˆ¹æ¸¯ï¼‰ã®æ¬ æå€¤è£œå®Œ\n",
    "# æœ€é »å€¤ã§è£œå®Œ\n",
    "df_all['Embarked'] = df_all['Embarked'].fillna(df_all['Embarked'].mode()[0])\n",
    "\n",
    "print(\"\\n=== æ¬ æå€¤è£œå®Œå®Œäº† ===\")\n",
    "print(df_all.isnull().sum()[df_all.isnull().sum() > 0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Phase 6: è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "df_all['Age_Cat'] = pd.cut(df_all['Age'], \n",
    "                           bins=[0, 12, 18, 35, 60, 100],\n",
    "                           labels=['Child', 'Teen', 'Adult', 'MiddleAge', 'Senior'])\n",
    "\n",
    "# é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—\n",
    "df_all['Fare_Cat'] = pd.qcut(df_all['Fare'], q=5, labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'])\n",
    "\n",
    "# Sex-Pclassäº¤äº’ä½œç”¨\n",
    "df_all['Sex_Pclass'] = df_all['Sex'] + '_' + df_all['Pclass'].astype(str)\n",
    "\n",
    "# Age-Sexäº¤äº’ä½œç”¨\n",
    "df_all['Age_Sex'] = df_all['Age_Cat'].astype(str) + '_' + df_all['Sex']\n",
    "\n",
    "# Title-Pclassäº¤äº’ä½œç”¨\n",
    "df_all['Title_Pclass'] = df_all['Title_Grouped'].astype(str) + '_' + df_all['Pclass'].astype(str)\n",
    "\n",
    "# ç”Ÿå­˜å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼‰\n",
    "df_all['Priority_Score'] = 0\n",
    "df_all.loc[df_all['Sex'] == 'female', 'Priority_Score'] += 100\n",
    "df_all.loc[df_all['Is_Child'] == 1, 'Priority_Score'] += 80\n",
    "df_all.loc[df_all['Pclass'] == 1, 'Priority_Score'] += 30\n",
    "df_all.loc[df_all['Pclass'] == 2, 'Priority_Score'] += 15\n",
    "df_all.loc[df_all['Is_Mother'] == 1, 'Priority_Score'] += 20\n",
    "\n",
    "# ç¤¾ä¼šçµŒæ¸ˆçš„åœ°ä½ã‚¹ã‚³ã‚¢\n",
    "df_all['SES_Score'] = (\n",
    "    (4 - df_all['Pclass']) * 30 +  # ã‚¯ãƒ©ã‚¹ã®é€†æ•°\n",
    "    df_all['Fare_Per_Person'].rank(pct=True) * 100 +  # é‹è³ƒãƒ©ãƒ³ã‚¯\n",
    "    df_all['Has_Cabin'] * 50  # å®¢å®¤ä¿æœ‰\n",
    ")\n",
    "\n",
    "# åå‰ã®é•·ã•ï¼ˆç¤¾ä¼šçš„åœ°ä½ã®ä»£ç†æŒ‡æ¨™ï¼‰\n",
    "df_all['Name_Length'] = df_all['Name'].str.len()\n",
    "\n",
    "# ãƒã‚±ãƒƒãƒˆæ–‡å­—åˆ—ã®é•·ã•\n",
    "df_all['Ticket_Length'] = df_all['Ticket'].str.len()\n",
    "\n",
    "print(\"=== è¿½åŠ ç‰¹å¾´é‡ä½œæˆå®Œäº† ===\")\n",
    "print(f\"\\nä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len(df_all.columns)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Phase 7: ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "categorical_cols = [\n",
    "    'Sex', 'Embarked', 'Title_Grouped', 'Deck', 'Cabin_Position',\n",
    "    'Ticket_Prefix_Grouped', 'FamilySize_Cat', 'Family_Type',\n",
    "    'Age_Cat', 'Fare_Cat', 'Sex_Pclass', 'Age_Sex', 'Title_Pclass'\n",
    "]\n",
    "\n",
    "# Label Encoding\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df_all.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_all[col + '_Encoded'] = le.fit_transform(df_all[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(\"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†\")\n",
    "print(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå¤‰æ•°: {len(categorical_cols)}å€‹\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Phase 8: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²\n",
    "train_processed = df_all[df_all['is_train']].copy()\n",
    "test_processed = df_all[~df_all['is_train']].copy()\n",
    "\n",
    "# ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®é¸æŠ\n",
    "exclude_cols = [\n",
    "    'PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived', 'is_train',\n",
    "    'Surname', 'Title', 'Ticket_Cleaned', 'Ticket_Number',\n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã‚’ä½¿ç”¨ï¼‰\n",
    "    'Sex', 'Embarked', 'Title_Grouped', 'Deck', 'Cabin_Position',\n",
    "    'Ticket_Prefix', 'Ticket_Prefix_Grouped', 'FamilySize_Cat', 'Family_Type',\n",
    "    'Age_Cat', 'Fare_Cat', 'Sex_Pclass', 'Age_Sex', 'Title_Pclass'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in df_all.columns \n",
    "                if col not in exclude_cols and \n",
    "                df_all[col].dtype in ['int64', 'float64', 'int32', 'float32', 'int8', 'int16']]\n",
    "\n",
    "# NaNå‡¦ç†\n",
    "for col in feature_cols:\n",
    "    if train_processed[col].isnull().any():\n",
    "        train_processed[col] = train_processed[col].fillna(train_processed[col].median())\n",
    "        test_processed[col] = test_processed[col].fillna(train_processed[col].median())\n",
    "\n",
    "X_train = train_processed[feature_cols]\n",
    "y_train = train_processed['Survived']\n",
    "X_test = test_processed[feature_cols]\n",
    "\n",
    "print(f\"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "print(f\"\\nTop 20ç‰¹å¾´é‡:\")\n",
    "for i, col in enumerate(feature_cols[:20], 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LightGBMãƒ¢ãƒ‡ãƒ«ï¼ˆexp004ã®è¨­å®šãƒ™ãƒ¼ã‚¹ + æ”¹è‰¯ï¼‰\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,\n",
    "    'min_split_gain': 0.01,\n",
    "    'min_child_weight': 0.001,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 1000,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(X_train))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "feature_importance = np.zeros(len(feature_cols))\n",
    "\n",
    "print(\"\\n=== 5-Fold Cross Validation ===\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    val_pred_proba = model.predict(X_val)\n",
    "    val_pred = (val_pred_proba >= 0.5).astype(int)\n",
    "    test_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "    fold_score = accuracy_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    \n",
    "    # äºˆæ¸¬å€¤ä¿å­˜\n",
    "    oof_predictions[val_idx] = val_pred_proba\n",
    "    test_predictions += test_pred_proba / 5\n",
    "    \n",
    "    # ç‰¹å¾´é‡é‡è¦åº¦\n",
    "    feature_importance += model.feature_importances_ / 5\n",
    "    \n",
    "    print(f\"Fold {fold}: {fold_score:.4f} (trees: {model.n_estimators_})\")\n",
    "\n",
    "# çµæœã‚µãƒãƒªãƒ¼\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "oof_score = accuracy_score(y_train, (oof_predictions >= 0.5).astype(int))\n",
    "\n",
    "print(f\"\\n=== Cross Validationçµæœ ===\")\n",
    "print(f\"CV Mean: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "print(f\"OOF Score: {oof_score:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== Top 20 é‡è¦ç‰¹å¾´é‡ ===\")\n",
    "for i, row in importance_df.head(20).iterrows():\n",
    "    print(f\"{importance_df.index.get_loc(i)+1:2d}. {row['feature']:30s}: {row['importance']:8.2f}\")\n",
    "\n",
    "# æ–°è¦ç‰¹å¾´é‡ã®é‡è¦åº¦ç¢ºèª\n",
    "new_features = [\n",
    "    'Priority_Score', 'SES_Score', 'Surname_Survival_Rate', \n",
    "    'Ticket_Group_Size', 'Fare_Per_Person', 'Deck_Num',\n",
    "    'Is_Mother', 'Is_Father', 'Is_Child'\n",
    "]\n",
    "\n",
    "print(\"\\n=== æ–°è¦ç‰¹å¾´é‡ã®é‡è¦åº¦ ===\")\n",
    "for feat in new_features:\n",
    "    if feat in importance_df['feature'].values:\n",
    "        rank = importance_df.index.get_loc(importance_df[importance_df['feature'] == feat].index[0]) + 1\n",
    "        imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n",
    "        print(f\"Rank {rank:3d}: {feat:25s}: {imp:8.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Phase 9: çµæœåˆ†æã¨æå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# éå»å®Ÿé¨“ã¨ã®æ¯”è¼ƒ\n",
    "past_results = {\n",
    "    'exp001': {'cv': 0.8496, 'kaggle': 0.77272, 'features': 16},\n",
    "    'exp004': {'cv': 0.8462, 'kaggle': 0.77990, 'features': 23},\n",
    "    'exp006': {'cv': 0.8440, 'kaggle': 0.77272, 'features': 15},\n",
    "    'exp007': {'cv': None, 'kaggle': 0.77751, 'features': 25}\n",
    "}\n",
    "\n",
    "print(\"=== éå»å®Ÿé¨“ã¨ã®æ¯”è¼ƒ ===\")\n",
    "for exp, results in past_results.items():\n",
    "    print(f\"{exp}: CV={results['cv']}, Kaggle={results['kaggle']:.5f}, Features={results['features']}\")\n",
    "\n",
    "print(f\"\\nexp008 (Advanced FE):\")\n",
    "print(f\"  CV Score: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "\n",
    "# exp004åŸºæº–ã§ã®æœŸå¾…å€¤è¨ˆç®—\n",
    "exp004_ratio = past_results['exp004']['kaggle'] / past_results['exp004']['cv']\n",
    "expected_kaggle = cv_mean * exp004_ratio\n",
    "print(f\"  æœŸå¾…Kaggle Score: {expected_kaggle:.5f}\")\n",
    "\n",
    "if cv_mean > past_results['exp004']['cv']:\n",
    "    print(f\"\\nğŸ‰ exp004ã®CVã‚’ä¸Šå›ã£ãŸï¼ (+{cv_mean - past_results['exp004']['cv']:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š exp004ã®CVã«ã¯åŠã°ãš ({cv_mean - past_results['exp004']['cv']:.4f})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': (test_predictions >= 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# äºˆæ¸¬åˆ†å¸ƒã®ç¢ºèª\n",
    "print(\"=== äºˆæ¸¬åˆ†å¸ƒ ===\")\n",
    "print(f\"ç”Ÿå­˜äºˆæ¸¬: {submission['Survived'].sum()} ({submission['Survived'].mean():.1%})\")\n",
    "print(f\"æ­»äº¡äºˆæ¸¬: {len(submission) - submission['Survived'].sum()} ({1 - submission['Survived'].mean():.1%})\")\n",
    "print(f\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿå­˜ç‡: {y_train.mean():.1%}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿå­˜ç‡: {submission['Survived'].mean():.1%}\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "import os\n",
    "os.makedirs('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp008', exist_ok=True)\n",
    "submission.to_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp008/result.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†\")\n",
    "print(f\"Path: results/exp008/result.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å®Ÿé¨“ã‚µãƒãƒªãƒ¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           ğŸš€ EXP008 ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š æœ€çµ‚çµæœ:\")\n",
    "print(f\"  CV Score: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "print(f\"  OOF Score: {oof_score:.4f}\")\n",
    "print(f\"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nğŸŒŸ ä¸»è¦ãªæ–°ç‰¹å¾´é‡:\")\n",
    "print(f\"  â€¢ Titleè©³ç´°åˆ†é¡ï¼ˆOfficer, Royalty, Dr, Revï¼‰\")\n",
    "print(f\"  â€¢ Cabinè©³ç´°ï¼ˆãƒ‡ãƒƒã‚­ã€ä½ç½®ã€è¤‡æ•°å®¢å®¤ï¼‰\")\n",
    "print(f\"  â€¢ Ticketåˆ†æï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã€ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºï¼‰\")\n",
    "print(f\"  â€¢ Familyè©³ç´°ï¼ˆMother, Father, Family Typeï¼‰\")\n",
    "print(f\"  â€¢ é«˜åº¦ãªæ¬ æå€¤è£œå®Œï¼ˆRandomForestäºˆæ¸¬ï¼‰\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ æŠ€è¡“çš„æ”¹å–„ç‚¹:\")\n",
    "print(f\"  â€¢ {len(feature_cols)}å€‹ã®ç‰¹å¾´é‡ï¼ˆéå»æœ€å¤šï¼‰\")\n",
    "print(f\"  â€¢ å¤šæ§˜ãªäº¤äº’ä½œç”¨ç‰¹å¾´é‡\")\n",
    "print(f\"  â€¢ ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã¨çµ±è¨ˆæ‰‹æ³•ã®èåˆ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœŸå¾…Kaggleã‚¹ã‚³ã‚¢: {expected_kaggle:.5f}\")\n",
    "if expected_kaggle > past_results['exp004']['kaggle']:\n",
    "    improvement = expected_kaggle - past_results['exp004']['kaggle']\n",
    "    print(f\"  â†’ exp004ã‚’ {improvement:.5f} ä¸Šå›ã‚‹è¦‹è¾¼ã¿ï¼ğŸ‰\")\n",
    "    print(f\"  â†’ 0.78ã®å£çªç ´ã®å¯èƒ½æ€§ï¼\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  Advanced Feature Engineering - The Devil is in the Details!\")\n",
    "print(\"  Kaggleæå‡ºã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™...\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}