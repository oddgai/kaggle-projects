# exp005 - アンサンブル手法による0.8突破への挑戦（失敗分析）

## 概要

exp004の成果（Kaggleスコア: 0.77990）を基に、アンサンブル手法による0.8突破を目指したが、結果的にexp002・exp003と同じ0.76315まで悪化した実験。CV精度は0.8507と高かったが、過学習により汎化性能が大幅に低下した。

## ベースとなる実験

exp004（高度な特徴量エンジニアリング、Kaggleスコア: 0.77990）

## 実験設定

### 使用手法

**4段階アプローチ**:
1. **強化特徴量エンジニアリング** - exp004の23特徴量から45特徴量に拡張
2. **多様性アンサンブル** - LightGBM + XGBoost + CatBoost
3. **基本アンサンブル最適化** - 4つのアンサンブル手法を比較
4. **最終予測** - 最高性能手法（Stacking）を選択

### 特徴量エンジニアリング詳細

#### exp004からの追加特徴量（22個）
1. **3次交互作用特徴量**
   - Sex × Pclass × Age_Group（40カテゴリ）
   - Title × Pclass × Embarked（37カテゴリ）

2. **PassengerId順序特徴量**
   - PassengerId_Rank（乗船順序の正規化）
   - PassengerId_Group（10グループ分割）

3. **客室関連高度特徴量**
   - Deck_Class_Interaction（15カテゴリ）
   - Deck_Age_Interaction（45カテゴリ）

4. **統計的偏差特徴量**
   - Age_vs_TitleMean（敬称平均からの年齢偏差）
   - Fare_vs_TitleMean（敬称平均からの運賃偏差）

5. **家族内順位特徴量**
   - Age_Rank_in_Family（家族内年齢順位）
   - Fare_Rank_in_Family（家族内運賃順位）

6. **高次統計・交互作用特徴量**
   - Age_Fare_Ratio、Fare_Per_Family
   - Sex_Embarked、Title_FamilySize等

### モデル設定

#### LightGBM
```python
lgb_params = {
    'objective': 'binary',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'reg_alpha': 0.1, 'reg_lambda': 0.1
}
```

#### XGBoost
```python
xgb_params = {
    'objective': 'binary:logistic',
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 0.1, 'reg_lambda': 1.0
}
```

#### CatBoost
```python
cb_params = {
    'learning_rate': 0.05,
    'depth': 6,
    'l2_leaf_reg': 3,
    'bootstrap_type': 'Bernoulli',
    'subsample': 0.8
}
```

### アンサンブル手法

5-fold StratifiedKFoldで以下4手法を比較:
1. **Simple Average**: 3モデルの単純平均
2. **Weighted Average**: OOF性能による重み付き平均  
3. **Best Single**: 最高性能単一モデル
4. **Stacking**: ロジスティック回帰メタモデル

## 結果

### 性能指標

#### 個別モデル性能
- **LightGBM**: CV=0.8485±0.0063, OOF=0.8485
- **XGBoost**: CV=0.8451±0.0118, OOF=0.8451  
- **CatBoost**: CV=0.8440±0.0201, OOF=0.8440

#### アンサンブル性能
- **Simple Average**: OOF=0.8496
- **Weighted Average**: OOF=0.8496
- **Best Single**: OOF=0.8485
- **Stacking**: OOF=0.8507（最高性能）

#### 最終結果
- **Kaggleスコア**: 0.76315（exp004から-0.01675の大幅悪化）
- **CV精度**: 0.8507（exp004から+0.0056の改善）
- **CV vs Kaggle乖離**: 0.0876（非常に大きな過学習）

## 失敗原因の詳細分析

### 1. 過度な特徴量エンジニアリング

#### 問題点
- **特徴量数の急激な増加**: 23個 → 45個（約2倍）
- **高次元カテゴリカル特徴量**: 最大45カテゴリまで細分化
- **情報量の希薄化**: 891サンプルに対して45特徴量は過剰

#### 具体的影響
```
カテゴリ数の詳細:
- Sex_Pclass_AgeGroup: 40カテゴリ
- Deck_Age_Interaction: 45カテゴリ  
- Title_FamilySize: 39カテゴリ
- Title_Pclass_Embarked: 37カテゴリ

→ 訓練データ891件に対して過度な細分化
→ 各カテゴリの平均サンプル数: 891/45 ≈ 20件未満
```

### 2. CV vs Kaggle精度の致命的乖離

#### 乖離の推移
- exp001: CV=0.8496, Kaggle=0.77272 → 差=0.0769
- exp004: CV=0.8462, Kaggle=0.77990 → 差=0.0663  
- **exp005: CV=0.8507, Kaggle=0.76315 → 差=0.0876**

#### 乖離拡大の要因
1. **訓練データへの過剰適応**: 複雑な特徴量により訓練データの特異パターンを学習
2. **アンサンブルによる過学習増幅**: 複数モデルが同じ過学習パターンを共有
3. **交差検証の限界**: 5-foldでは検出できない汎化性能の低下

### 3. アンサンブル手法の設計問題

#### 多様性の不足
- **特徴量**: 全モデルで同じ45特徴量を使用
- **前処理**: 同一の特徴量エンジニアリング
- **データ分割**: 同一のCV分割

#### 結果
```
モデル間相関（OOF予測）:
         LightGBM  XGBoost  CatBoost
LightGBM   1.0000   0.9439    0.9124
XGBoost    0.9439   1.0000    0.9173  
CatBoost   0.9124   0.9173    1.0000

→ 非常に高い相関（0.91-0.94）
→ アンサンブルの多様性効果が限定的
```

### 4. 統計的問題

#### 小標本問題
- **訓練データ**: 891サンプル
- **特徴量数**: 45個
- **サンプル/特徴量比**: 19.8（推奨値50-100を大幅に下回る）

#### カテゴリカル特徴量の希薄性
```
例：Deck_Age_Interaction（45カテゴリ）
→ 平均19.8件/カテゴリ
→ 多くのカテゴリで5件未満のサンプル
→ 統計的に不安定な推定
```

## 教訓と学習事項

### ❌ 失敗した手法
1. **特徴量の量的拡張**: より多くの特徴量≠より良い性能
2. **カテゴリの細分化**: 高次元カテゴリカル特徴量は過学習の原因
3. **同質アンサンブル**: 同じ特徴量・前処理での多モデルは多様性不足
4. **CV精度への過信**: CV改善が汎化性能改善を保証しない

### ✅ 成功した技術要素
1. **個別モデルの実装**: LightGBM、XGBoost、CatBoostの正常動作
2. **Stackingの実装**: メタモデルによるアンサンブル最適化
3. **系統的評価**: 複数アンサンブル手法の比較分析
4. **詳細なログ記録**: 失敗原因特定のための充実したログ

### 📊 重要な気づき

#### Titanicデータセットの特性
- **小規模データ**: 891サンプルでは複雑な特徴量工学は逆効果
- **ドメイン制約**: 歴史的事実に基づく特徴量以外は過学習リスク
- **シンプルな関係性**: 基本的な特徴量の組み合わせが最も効果的

#### 汎化性能の重要性
- **CV精度の罠**: 高いCV精度は必ずしも良い汎化性能を示さない
- **リークの危険性**: train+testを使った統計量計算の潜在的リスク
- **特徴量選択の重要性**: 質の高い少数特徴量 > 大量の特徴量

## 次の実験に向けた改善提案

### 短期的改善案（exp006候補）

#### 1. 特徴量削減アプローチ
- exp004の23特徴量から**重要度下位を削除**
- **15-20特徴量**に絞り込み
- カテゴリカル特徴量の**粒度を粗く**調整

#### 2. 単一モデル最適化
- LightGBMでの**ハイパーパラメータ最適化**
- **正則化強化**（reg_alpha, reg_lambda増加）
- **Early Stopping**の厳格化

#### 3. バリデーション戦略改善
- **Stratified GroupKFold**でより厳格な検証
- **TimeSeriesSplit**による時系列考慮（PassengerId順）

### 中長期的改善案

#### 1. 異質アンサンブル
- **異なる特徴量セット**を各モデルに適用
- **異なる前処理**パイプライン
- **Neural Network**の追加による異質性向上

#### 2. ドメイン知識活用
- **歴史的事実**に基づく特徴量のみに限定
- **物理的制約**を考慮した特徴量設計
- **社会学的観点**からの特徴量解釈

#### 3. 外部データ活用
- **船の構造図**に基づく客室位置データ
- **乗客の社会的背景**データ
- **当時の社会情勢**データ

## 結論

exp005は技術的実装は成功したが、**機械学習の根本的原理**（汎化性能の重要性、過学習の回避、特徴量選択の重要性）を軽視した結果、大幅な性能悪化を招いた。

### 核心的教訓
1. **「より複雑」は「より良い」ではない**
2. **CV精度は汎化性能の近似でしかない**
3. **小規模データでは特に単純性が重要**
4. **ドメイン知識が統計的手法を補完する**

この失敗により、機械学習における**質的改善の重要性**と**量的拡張の限界**を深く理解できた貴重な学習機会となった。

---

**次回exp006では、この失敗を踏まえて「シンプルで効果的」なアプローチに回帰し、exp004を超える成果を目指す。**