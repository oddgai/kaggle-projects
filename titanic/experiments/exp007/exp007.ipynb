{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp007 - Domain Expert Strategy: ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®æ­´å²çš„äº‹å®Ÿã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "## ğŸ¯ ç›®æ¨™\n",
    "- ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·äº‹æ•…ã®æ­´å²çš„äº‹å®Ÿã¨ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ç”¨ã—ãŸç‰¹å¾´é‡è¨­è¨ˆ\n",
    "- ã€ŒWomen and Children Firstã€åŸå‰‡ã®æ•°å€¤åŒ–\n",
    "- ç‰©ç†çš„é…ç½®ï¼ˆãƒ‡ãƒƒã‚­ãƒ»å®¢å®¤ä½ç½®ï¼‰ã«ã‚ˆã‚‹ç”Ÿå­˜ç‡ã¸ã®å½±éŸ¿ã‚’åæ˜ \n",
    "- exp004ï¼ˆ0.77990ï¼‰ã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç›®æŒ‡ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "\n",
    "print(\"ğŸš¢ exp007 - Domain Expert Strategy\")\n",
    "print(\"ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®æ­´å²çš„äº‹å®Ÿã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "train_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/train.csv')\n",
    "test_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/test.csv')\n",
    "\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Phase 1: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ•´ç†\n",
    "\n",
    "### ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®é‡è¦ãªäº‹å®Ÿ\n",
    "1. **æ•‘å‘½ãƒœãƒ¼ãƒˆ**: å…¨å“¡åˆ†ãªã—ï¼ˆ2,224äººä¸­1,178äººåˆ†ã®ã¿ï¼‰\n",
    "2. **é¿é›£åŸå‰‡**: ã€ŒWomen and Children Firstã€\n",
    "3. **ãƒ‡ãƒƒã‚­æ§‹é€ **: ä¸Šå±¤ãƒ‡ãƒƒã‚­ã»ã©æ•‘å‘½ãƒœãƒ¼ãƒˆã«è¿‘ã„\n",
    "4. **ç¤¾ä¼šéšç´š**: 1ç­‰å®¢å®¤ã¯ä¸Šå±¤ãƒ‡ãƒƒã‚­ã€3ç­‰ã¯ä¸‹å±¤\n",
    "5. **å®¶æ—å˜ä½**: å®¶æ—ã¯ä¸€ç·’ã«è¡Œå‹•ã™ã‚‹å‚¾å‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®ãƒ‡ãƒƒã‚­æ§‹é€ åˆ†æ\n",
    "print(\"=== ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®ãƒ‡ãƒƒã‚­æ§‹é€ åˆ†æ ===\")\n",
    "print(\"\\nå®¢å®¤ç•ªå·ã‹ã‚‰ãƒ‡ãƒƒã‚­ã‚’æ¨å®š:\")\n",
    "\n",
    "# å®¢å®¤ç•ªå·ã®æœ€åˆã®æ–‡å­—ãŒãƒ‡ãƒƒã‚­ã‚’ç¤ºã™\n",
    "train_cabin_sample = train_df[train_df['Cabin'].notna()]['Cabin'].head(20)\n",
    "for cabin in train_cabin_sample:\n",
    "    deck = cabin[0] if pd.notna(cabin) else 'Unknown'\n",
    "    print(f\"  {cabin} â†’ ãƒ‡ãƒƒã‚­ {deck}\")\n",
    "\n",
    "# ãƒ‡ãƒƒã‚­åˆ¥ã®ç”Ÿå­˜ç‡\n",
    "train_with_cabin = train_df[train_df['Cabin'].notna()].copy()\n",
    "train_with_cabin['Deck'] = train_with_cabin['Cabin'].str[0]\n",
    "\n",
    "deck_survival = train_with_cabin.groupby('Deck')['Survived'].agg(['mean', 'count']).round(3)\n",
    "print(\"\\n=== ãƒ‡ãƒƒã‚­åˆ¥ç”Ÿå­˜ç‡ ===\")\n",
    "print(deck_survival.sort_values('mean', ascending=False))\n",
    "\n",
    "# å¹´é½¢ãƒ»æ€§åˆ¥åˆ¥ã®ç”Ÿå­˜ç‡ï¼ˆWomen and Children FirståŸå‰‡ã®ç¢ºèªï¼‰\n",
    "print(\"\\n=== Women and Children FirståŸå‰‡ã®ç¢ºèª ===\")\n",
    "women_survival = train_df[train_df['Sex'] == 'female']['Survived'].mean()\n",
    "men_survival = train_df[train_df['Sex'] == 'male']['Survived'].mean()\n",
    "children_survival = train_df[train_df['Age'] < 16]['Survived'].mean()\n",
    "\n",
    "print(f\"å¥³æ€§ã®ç”Ÿå­˜ç‡: {women_survival:.1%}\")\n",
    "print(f\"ç”·æ€§ã®ç”Ÿå­˜ç‡: {men_survival:.1%}\")\n",
    "print(f\"å­ä¾›ï¼ˆ16æ­³æœªæº€ï¼‰ã®ç”Ÿå­˜ç‡: {children_survival:.1%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Phase 2: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_domain_features(df):\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ========== åŸºæœ¬å‰å‡¦ç†ï¼ˆexp004ãƒ™ãƒ¼ã‚¹ï¼‰ ==========\n",
    "    # åå‰ã‹ã‚‰ç§°å·æŠ½å‡º\n",
    "    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.')\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "        'Dr': 'Professional', 'Rev': 'Professional', 'Col': 'Military', \n",
    "        'Major': 'Military', 'Capt': 'Military',\n",
    "        'Mlle': 'Miss', 'Ms': 'Mrs', 'Mme': 'Mrs',\n",
    "        'Lady': 'Noble', 'Countess': 'Noble', 'Sir': 'Noble', \n",
    "        'Don': 'Noble', 'Dona': 'Noble', 'Jonkheer': 'Noble'\n",
    "    }\n",
    "    df['Title_Grouped'] = df['Title'].map(title_mapping).fillna('Other')\n",
    "    \n",
    "    # åŸºæœ¬ç‰¹å¾´é‡ã®å‰å‡¦ç†\n",
    "    df['Sex_Binary'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "    df['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # å®¶æ—æ§‹æˆ\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # ========== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ 1: æ•‘åŠ©å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ ==========\n",
    "    # \"Women and Children First\" åŸå‰‡ã®æ•°å€¤åŒ–\n",
    "    df['Rescue_Priority'] = 0\n",
    "    \n",
    "    # æœ€å„ªå…ˆ: å¥³æ€§ã¨å­ä¾›\n",
    "    df.loc[(df['Sex'] == 'female'), 'Rescue_Priority'] += 100\n",
    "    df.loc[(df['Age'] < 16), 'Rescue_Priority'] += 80\n",
    "    \n",
    "    # æ¬¡å„ªå…ˆ: æ¯è¦ªï¼ˆå¥³æ€§ã§å­ä¾›åŒä¼´ï¼‰\n",
    "    df.loc[(df['Sex'] == 'female') & (df['Parch'] > 0), 'Rescue_Priority'] += 20\n",
    "    \n",
    "    # ç¤¾ä¼šçš„åœ°ä½ã«ã‚ˆã‚‹èª¿æ•´\n",
    "    df.loc[df['Pclass'] == 1, 'Rescue_Priority'] += 15\n",
    "    df.loc[df['Pclass'] == 2, 'Rescue_Priority'] += 5\n",
    "    \n",
    "    # é«˜é½¢è€…ã¯å„ªå…ˆåº¦ä½ä¸‹\n",
    "    df.loc[(df['Sex'] == 'male') & (df['Age'] > 60), 'Rescue_Priority'] -= 10\n",
    "    \n",
    "    # ========== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ 2: ç‰©ç†çš„ä½ç½®ã‚¹ã‚³ã‚¢ ==========\n",
    "    # ãƒ‡ãƒƒã‚­ä½ç½®ï¼ˆå®¢å®¤ç•ªå·ã‹ã‚‰æ¨å®šï¼‰\n",
    "    df['Deck'] = df['Cabin'].str[0] if 'Cabin' in df.columns else None\n",
    "    \n",
    "    # ãƒ‡ãƒƒã‚­ã®æ•‘å‘½ãƒœãƒ¼ãƒˆè¿‘æ¥åº¦ã‚¹ã‚³ã‚¢\n",
    "    deck_score = {\n",
    "        'A': 100, 'B': 90, 'C': 80, 'D': 70, \n",
    "        'E': 60, 'F': 50, 'G': 40, 'T': 30\n",
    "    }\n",
    "    df['Deck_Score'] = df['Deck'].map(deck_score).fillna(0)\n",
    "    \n",
    "    # å®¢å®¤ã®æœ‰ç„¡ï¼ˆä¸Šå±¤ãƒ‡ãƒƒã‚­ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼‰\n",
    "    df['HasCabin'] = (~df['Cabin'].isnull()).astype(int)\n",
    "    \n",
    "    # Pclassã¨ãƒ‡ãƒƒã‚­ã‚¹ã‚³ã‚¢ã®çµ„ã¿åˆã‚ã›ï¼ˆæ¨å®šä½ç½®ã‚¹ã‚³ã‚¢ï¼‰\n",
    "    df['Location_Score'] = df['Deck_Score'] + (4 - df['Pclass']) * 20\n",
    "    \n",
    "    # ========== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ 3: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ ==========\n",
    "    # è‹—å­—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆå®¶æ—ãƒ»åŒè¡Œè€…ï¼‰\n",
    "    df['Surname'] = df['Name'].str.split(',').str[0]\n",
    "    \n",
    "    # åŒä¸€ãƒã‚±ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º\n",
    "    ticket_groups = pd.concat([train_df['Ticket'], test_df['Ticket']]).value_counts()\n",
    "    df['Ticket_Group_Size'] = df['Ticket'].map(ticket_groups)\n",
    "    \n",
    "    # åŒä¸€è‹—å­—ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º\n",
    "    surname_groups = pd.concat([train_df['Name'], test_df['Name']]).str.split(',').str[0].value_counts()\n",
    "    df['Surname_Group_Size'] = df['Surname'].map(surname_groups)\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—ï¼ˆå®¶æ— vs åŒè¡Œè€…ï¼‰\n",
    "    df['Is_Family_Group'] = ((df['FamilySize'] > 1) & \n",
    "                             (df['Surname_Group_Size'] == df['FamilySize'])).astype(int)\n",
    "    \n",
    "    # ========== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ 4: ç¤¾ä¼šçµŒæ¸ˆçš„æŒ‡æ¨™ ==========\n",
    "    # é‹è³ƒã®ç›¸å¯¾çš„ä½ç½®ï¼ˆåŒã‚¯ãƒ©ã‚¹å†…ï¼‰\n",
    "    df['Fare_Percentile'] = df.groupby('Pclass')['Fare'].rank(pct=True)\n",
    "    \n",
    "    # é«˜é¡ãƒã‚±ãƒƒãƒˆä¿æŒè€…ãƒ•ãƒ©ã‚°\n",
    "    fare_threshold = df.groupby('Pclass')['Fare'].transform(lambda x: x.quantile(0.8))\n",
    "    df['Is_High_Fare'] = (df['Fare'] > fare_threshold).astype(int)\n",
    "    \n",
    "    # ãƒã‚±ãƒƒãƒˆç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•°å­—ã®ã¿ vs æ–‡å­—å«ã‚€ï¼‰\n",
    "    df['Ticket_IsNumeric'] = df['Ticket'].str.isnumeric().astype(int)\n",
    "    \n",
    "    # ========== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ 5: è¤‡åˆæŒ‡æ¨™ ==========\n",
    "    # ç·åˆç”Ÿå­˜å¯èƒ½æ€§ã‚¹ã‚³ã‚¢\n",
    "    df['Survival_Score'] = (\n",
    "        df['Rescue_Priority'] * 0.4 +  # æ•‘åŠ©å„ªå…ˆåº¦\n",
    "        df['Location_Score'] * 0.3 +    # ç‰©ç†çš„ä½ç½®\n",
    "        df['Is_High_Fare'] * 30 +       # çµŒæ¸ˆåŠ›\n",
    "        (1 - df['IsAlone']) * 20        # ã‚°ãƒ«ãƒ¼ãƒ—åŠ¹æœ\n",
    "    )\n",
    "    \n",
    "    # å±é™ºåº¦ã‚¹ã‚³ã‚¢ï¼ˆé€†æŒ‡æ¨™ï¼‰\n",
    "    df['Risk_Score'] = (\n",
    "        df['Sex_Binary'] * 100 +                    # ç”·æ€§\n",
    "        (df['Pclass'] == 3).astype(int) * 50 +     # 3ç­‰å®¢å®¤\n",
    "        df['IsAlone'] * 30 +                        # å˜ç‹¬\n",
    "        (df['Age'] > 50).astype(int) * 20           # é«˜é½¢\n",
    "    )\n",
    "    \n",
    "    # ========== å¾“æ¥ã®æœ‰åŠ¹ç‰¹å¾´é‡ï¼ˆexp004ã‹ã‚‰ï¼‰ ==========\n",
    "    # äº¤äº’ä½œç”¨\n",
    "    df['Sex_Pclass'] = df['Sex_Binary'] * df['Pclass']\n",
    "    df['Age_Pclass'] = df['Age'] * df['Pclass']\n",
    "    \n",
    "    # å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆã‚ˆã‚Šç´°åˆ†åŒ–ï¼‰\n",
    "    df['Age_Group'] = pd.cut(df['Age'], \n",
    "                             bins=[0, 12, 18, 35, 50, 65, 100], \n",
    "                             labels=['Child', 'Teen', 'Young', 'Middle', 'Senior', 'Elder'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ç‰¹å¾´é‡ä½œæˆ\n",
    "print(\"ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "train_processed = create_domain_features(train_df)\n",
    "test_processed = create_domain_features(test_df)\n",
    "\n",
    "print(\"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†\")\n",
    "print(f\"\\nä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len([col for col in train_processed.columns if col not in train_df.columns])} å€‹\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "categorical_features = ['Embarked', 'Title_Grouped', 'Age_Group']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # train/teståˆã‚ã›ã¦fitã—ã¦ä¸€è²«æ€§ç¢ºä¿\n",
    "    all_values = pd.concat([train_processed[feature], test_processed[feature]]).astype(str)\n",
    "    le.fit(all_values)\n",
    "    \n",
    "    train_processed[feature + '_Encoded'] = le.transform(train_processed[feature].astype(str))\n",
    "    test_processed[feature + '_Encoded'] = le.transform(test_processed[feature].astype(str))\n",
    "\n",
    "print(\"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æ–°ç‰¹å¾´é‡ã®åˆ†æ\n",
    "print(\"=== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ã®çµ±è¨ˆ ===\")\n",
    "domain_features = ['Rescue_Priority', 'Location_Score', 'Survival_Score', 'Risk_Score']\n",
    "\n",
    "for feature in domain_features:\n",
    "    survived_mean = train_processed[train_processed['Survived'] == 1][feature].mean()\n",
    "    died_mean = train_processed[train_processed['Survived'] == 0][feature].mean()\n",
    "    diff = survived_mean - died_mean\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  ç”Ÿå­˜è€…å¹³å‡: {survived_mean:.2f}\")\n",
    "    print(f\"  æ­»äº¡è€…å¹³å‡: {died_mean:.2f}\")\n",
    "    print(f\"  å·®åˆ†: {diff:+.2f} {'âœ…' if abs(diff) > 10 else ''}\")\n",
    "\n",
    "# ç›¸é–¢åˆ†æ\n",
    "correlation_with_survival = train_processed[domain_features + ['Survived']].corr()['Survived'].drop('Survived')\n",
    "print(\"\\n=== ç”Ÿå­˜ã¨ã®ç›¸é–¢ ===\")\n",
    "print(correlation_with_survival.sort_values(ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Phase 3: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®é¸æŠ\n",
    "exclude_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived', \n",
    "                'Surname', 'Title', 'Sex', 'Embarked', 'Title_Grouped', \n",
    "                'Age_Group', 'Deck']\n",
    "\n",
    "feature_cols = [col for col in train_processed.columns \n",
    "                if col not in exclude_cols and \n",
    "                train_processed[col].dtype in ['int64', 'float64', 'int32', 'float32', 'int8']]\n",
    "\n",
    "X_train = train_processed[feature_cols]\n",
    "y_train = train_processed['Survived']\n",
    "X_test = test_processed[feature_cols]\n",
    "\n",
    "print(f\"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "print(f\"\\nç‰¹å¾´é‡ãƒªã‚¹ãƒˆ:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LightGBMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆexp004ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ + èª¿æ•´ï¼‰\n",
    "params = {\n",
    "    'objective': 'regression',  # exp004ã§æˆåŠŸã—ãŸè¨­å®š\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(X_train))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "feature_importance = np.zeros(len(feature_cols))\n",
    "\n",
    "print(\"\\n5-Fold Cross Validationé–‹å§‹...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # äºˆæ¸¬ï¼ˆå›å¸°ãªã®ã§é–¾å€¤0.5ã§2å€¤åŒ–ï¼‰\n",
    "    val_pred_proba = model.predict(X_val)\n",
    "    val_pred = (val_pred_proba >= 0.5).astype(int)\n",
    "    test_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "    fold_score = accuracy_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    \n",
    "    # äºˆæ¸¬å€¤ä¿å­˜\n",
    "    oof_predictions[val_idx] = val_pred_proba\n",
    "    test_predictions += test_pred_proba / 5\n",
    "    \n",
    "    # ç‰¹å¾´é‡é‡è¦åº¦\n",
    "    feature_importance += model.feature_importances_ / 5\n",
    "    \n",
    "    print(f\"Fold {fold}: {fold_score:.4f}\")\n",
    "\n",
    "# çµæœã‚µãƒãƒªãƒ¼\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "oof_score = accuracy_score(y_train, (oof_predictions >= 0.5).astype(int))\n",
    "\n",
    "print(f\"\\n=== Cross Validationçµæœ ===\")\n",
    "print(f\"CV Mean: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "print(f\"OOF Score: {oof_score:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===\")\n",
    "print(\"\\nTop 15 é‡è¦ç‰¹å¾´é‡:\")\n",
    "for i, row in importance_df.head(15).iterrows():\n",
    "    is_domain = 'ğŸŒŸ' if row['feature'] in ['Rescue_Priority', 'Location_Score', \n",
    "                                           'Survival_Score', 'Risk_Score'] else '  '\n",
    "    print(f\"{is_domain} {importance_df.index.get_loc(i)+1:2d}. {row['feature']:25s}: {row['importance']:8.2f}\")\n",
    "\n",
    "# ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ã®é‡è¦åº¦\n",
    "domain_importance = importance_df[importance_df['feature'].isin(\n",
    "    ['Rescue_Priority', 'Location_Score', 'Survival_Score', 'Risk_Score',\n",
    "     'Ticket_Group_Size', 'Surname_Group_Size', 'Is_Family_Group']\n",
    ")]\n",
    "\n",
    "print(\"\\n=== ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ã®é‡è¦åº¦ ===\")\n",
    "for _, row in domain_importance.iterrows():\n",
    "    rank = importance_df.index.get_loc(importance_df[importance_df['feature'] == row['feature']].index[0]) + 1\n",
    "    print(f\"Rank {rank:2d}: {row['feature']:20s}: {row['importance']:8.2f}\")\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(20)\n",
    "colors = ['red' if feat in ['Rescue_Priority', 'Location_Score', 'Survival_Score', 'Risk_Score'] \n",
    "          else 'blue' for feat in top_features['feature']]\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('é‡è¦åº¦')\n",
    "plt.title('exp007 ç‰¹å¾´é‡é‡è¦åº¦ (èµ¤=ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Phase 4: çµæœåˆ†æã¨æå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# exp004ã¨ã®æ¯”è¼ƒ\n",
    "exp004_cv = 0.8462\n",
    "exp004_kaggle = 0.77990\n",
    "exp004_features = 23\n",
    "\n",
    "improvement = cv_mean - exp004_cv\n",
    "expected_kaggle = cv_mean * (exp004_kaggle / exp004_cv)  # æ¯”ä¾‹æ¨å®š\n",
    "\n",
    "print(\"=== exp004ã¨ã®æ¯”è¼ƒ ===\")\n",
    "print(f\"\\nexp004:\")\n",
    "print(f\"  CV Score: {exp004_cv:.4f}\")\n",
    "print(f\"  Kaggle Score: {exp004_kaggle:.5f}\")\n",
    "print(f\"  ç‰¹å¾´é‡æ•°: {exp004_features}\")\n",
    "\n",
    "print(f\"\\nexp007 (Domain Expert):\")\n",
    "print(f\"  CV Score: {cv_mean:.4f} ({improvement:+.4f})\")\n",
    "print(f\"  æœŸå¾…Kaggle Score: {expected_kaggle:.5f}\")\n",
    "print(f\"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\næ”¹å–„äºˆæ¸¬:\")\n",
    "if improvement > 0:\n",
    "    print(f\"  ğŸ‰ CVæ”¹å–„ {improvement:+.4f}\")\n",
    "    print(f\"  ğŸ“ˆ Kaggleäºˆæ¸¬: {expected_kaggle:.5f} (exp004ã‹ã‚‰ {expected_kaggle - exp004_kaggle:+.5f})\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ CVä½ä¸‹ {improvement:+.4f}\")\n",
    "    print(f\"  ğŸ“‰ è¦å› åˆ†æãŒå¿…è¦\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_processed['PassengerId'],\n",
    "    'Survived': (test_predictions >= 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# çµæœã®ç¢ºèª\n",
    "print(\"=== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒãƒªãƒ¼ ===\")\n",
    "print(f\"\\näºˆæ¸¬åˆ†å¸ƒ:\")\n",
    "print(f\"  ç”Ÿå­˜äºˆæ¸¬: {submission['Survived'].sum()} ({submission['Survived'].mean():.1%})\")\n",
    "print(f\"  æ­»äº¡äºˆæ¸¬: {len(submission) - submission['Survived'].sum()} ({1 - submission['Survived'].mean():.1%})\")\n",
    "\n",
    "print(f\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒ:\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿå­˜ç‡: {y_train.mean():.1%}\")\n",
    "print(f\"  ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿå­˜ç‡: {submission['Survived'].mean():.1%}\")\n",
    "print(f\"  å·®åˆ†: {submission['Survived'].mean() - y_train.mean():+.1%}\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "import os\n",
    "os.makedirs('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp007', exist_ok=True)\n",
    "submission.to_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp007/result.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†\")\n",
    "print(f\"Path: results/exp007/result.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å®Ÿé¨“ã‚µãƒãƒªãƒ¼\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"              ğŸš¢ EXP007 DOMAIN EXPERT STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“Š æœ€çµ‚çµæœ:\")\n",
    "print(f\"  CV Score: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "print(f\"  OOF Score: {oof_score:.4f}\")\n",
    "print(f\"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nğŸŒŸ ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ã®åŠ¹æœ:\")\n",
    "for feat in ['Rescue_Priority', 'Location_Score', 'Survival_Score', 'Risk_Score']:\n",
    "    if feat in importance_df['feature'].values:\n",
    "        rank = importance_df.index.get_loc(importance_df[importance_df['feature'] == feat].index[0]) + 1\n",
    "        imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n",
    "        print(f\"  {feat:20s}: Rank {rank:2d} (é‡è¦åº¦ {imp:.1f})\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ä¸»è¦ãªç™ºè¦‹:\")\n",
    "print(f\"  1. æ•‘åŠ©å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ãŒ{'æœ‰åŠ¹' if 'Rescue_Priority' in importance_df.head(10)['feature'].values else 'é™å®šçš„'}\")\n",
    "print(f\"  2. ç‰©ç†çš„ä½ç½®ã®å½±éŸ¿ãŒ{'å¼·ã„' if 'Location_Score' in importance_df.head(15)['feature'].values else 'å¼±ã„'}\")\n",
    "print(f\"  3. ã‚°ãƒ«ãƒ¼ãƒ—ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ãŒ{'é‡è¦' if 'Ticket_Group_Size' in importance_df.head(15)['feature'].values else 'è£œåŠ©çš„'}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœŸå¾…Kaggleã‚¹ã‚³ã‚¢: {expected_kaggle:.5f}\")\n",
    "if expected_kaggle > exp004_kaggle:\n",
    "    print(f\"  â†’ exp004ã‚’ {expected_kaggle - exp004_kaggle:.5f} ä¸Šå›ã‚‹è¦‹è¾¼ã¿ï¼\")\n",
    "else:\n",
    "    print(f\"  â†’ exp004ã«ã¯ {exp004_kaggle - expected_kaggle:.5f} åŠã°ãªã„å¯èƒ½æ€§\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  Domain knowledge makes the difference! ğŸ“\")\n",
    "print(\"  Kaggleæå‡ºã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™...\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}