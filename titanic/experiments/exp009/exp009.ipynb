{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp009 - Perfect Ensemble Strategy: 0.83の壁を突破せよ！\n",
    "\n",
    "## 🎯 目標\n",
    "- exp008（0.78468）から大幅改善\n",
    "- アンサンブル手法で0.83を目指す\n",
    "- 特徴量最適化 + Perfect Stacking + ハイパーパラメータ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer  # この行を追加\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport optuna\nimport re\n\nplt.rcParams['font.family'] = 'IPAexGothic'\nprint(\"🚀 exp009 - Perfect Ensemble Strategy\")\nprint(\"目標: 0.83の壁を突破！\")\n\n# データ読み込み\ntrain_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/train.csv')\ntest_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/test.csv')\n\n# データ結合\ndf_all = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\ndf_all['is_train'] = df_all['Survived'].notna()\n\nprint(f\"\\nデータ読み込み完了:\")\nprint(f\"  訓練データ: {df_all['is_train'].sum()}件\")\nprint(f\"  テストデータ: {(~df_all['is_train']).sum()}件\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Phase 1: 特徴量エンジニアリング（最適化版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_features(df):\n",
    "    \"\"\"最適化された特徴量エンジニアリング\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ========== Title処理 ==========\n",
    "    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "        'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss',\n",
    "        'Col': 'Officer', 'Major': 'Officer', 'Capt': 'Officer',\n",
    "        'Lady': 'Royalty', 'Sir': 'Royalty', 'Countess': 'Royalty',\n",
    "        'Don': 'Royalty', 'Dona': 'Royalty', 'Jonkheer': 'Royalty',\n",
    "        'Dr': 'Professional', 'Rev': 'Professional'\n",
    "    }\n",
    "    df['Title_Grouped'] = df['Title'].map(title_mapping).fillna('Other')\n",
    "\n",
    "    # ========== 基本前処理 ==========\n",
    "    df['Sex_Binary'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "    # ========== Cabin処理 ==========\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Deck'] = df['Cabin'].str[0]\n",
    "\n",
    "    deck_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1, 'T': 0}\n",
    "    df['Deck_Num'] = df['Deck'].map(deck_mapping)\n",
    "\n",
    "    # Pclassベースでデッキ推定\n",
    "    df.loc[(df['Pclass'] == 1) & df['Deck'].isna(), 'Deck_Num'] = 5\n",
    "    df.loc[(df['Pclass'] == 2) & df['Deck'].isna(), 'Deck_Num'] = 3\n",
    "    df.loc[(df['Pclass'] == 3) & df['Deck'].isna(), 'Deck_Num'] = 1\n",
    "    df['Deck_Num'] = df['Deck_Num'].fillna(0)\n",
    "\n",
    "    # ========== Ticket処理 ==========\n",
    "    ticket_counts = df['Ticket'].value_counts()\n",
    "    df['Ticket_Group_Size'] = df['Ticket'].map(ticket_counts)\n",
    "\n",
    "    df['Ticket_IsNumeric'] = df['Ticket'].str.isnumeric().astype(int)\n",
    "\n",
    "    # ========== 苗字・家族関係 ==========\n",
    "    df['Surname'] = df['Name'].str.split(',').str[0]\n",
    "    surname_counts = df['Surname'].value_counts()\n",
    "    df['Surname_Count'] = df['Surname'].map(surname_counts)\n",
    "\n",
    "    # 家族タイプ\n",
    "    df['Is_Mother'] = ((df['Sex'] == 'female') & (df['Parch'] > 0) & (df['Age'] > 18)).astype(int)\n",
    "    df['Is_Child'] = ((df['Age'] <= 16) | (df['Title'] == 'Master')).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 特徴量作成\n",
    "print(\"特徴量エンジニアリング実行中...\")\n",
    "df_processed = create_optimized_features(df_all)\n",
    "print(\"✅ 特徴量作成完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Phase 2: 高度な欠損値処理・外れ値処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== Age欠損値処理（IterativeImputer） ==========\n# Age予測用特徴量\nage_features = ['Pclass', 'Sex_Binary', 'SibSp', 'Parch', 'Fare', 'Has_Cabin']\n\n# カテゴリカル変数エンコード\nle_title = LabelEncoder()\ndf_processed['Title_Encoded'] = le_title.fit_transform(df_processed['Title_Grouped'])\nage_features.append('Title_Encoded')\n\nle_embarked = LabelEncoder()\ndf_processed['Embarked'] = df_processed['Embarked'].fillna('S')  # 最頻値補完\ndf_processed['Embarked_Encoded'] = le_embarked.fit_transform(df_processed['Embarked'])\nage_features.append('Embarked_Encoded')\n\n# Fare欠損値を先に処理\ndf_processed['Fare'] = df_processed.groupby(['Pclass', 'Embarked'])['Fare'].transform(\n    lambda x: x.fillna(x.median())\n)\ndf_processed['Fare'] = df_processed['Fare'].fillna(df_processed['Fare'].median())\n\n# Age予測\nprint(\"Age欠損値をIterativeImputerで補完中...\")\nage_imputer = IterativeImputer(random_state=42, max_iter=10)\nage_data = df_processed[age_features + ['Age']].copy()\nage_data_imputed = age_imputer.fit_transform(age_data)\ndf_processed['Age'] = age_data_imputed[:, -1]  # Age列\n\nprint(f\"Age補完完了: {df_processed['Age'].isnull().sum()}件の欠損\")\n\n# ========== 外れ値処理 ==========\n# Fareの外れ値処理（IQRベース）\ndef handle_outliers_iqr(df, column, factor=1.5):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - factor * IQR\n    upper_bound = Q3 + factor * IQR\n    \n    outliers_before = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n    print(f\"{column}の外れ値処理: {outliers_before}件をクリッピング\")\n    return df\n\ndf_processed = handle_outliers_iqr(df_processed, 'Fare')\ndf_processed = handle_outliers_iqr(df_processed, 'Age')\n\nprint(\"✅ 欠損値・外れ値処理完了\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Phase 3: 高品質特徴量の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 高品質特徴量の追加 ==========\n",
    "\n",
    "# 運賃の正規化\n",
    "df_processed['Fare_Per_Person'] = df_processed['Fare'] / df_processed['Ticket_Group_Size']\n",
    "\n",
    "# 年齢・運賃のランキング\n",
    "df_processed['Age_Rank'] = df_processed.groupby(['Sex', 'Pclass'])['Age'].rank(pct=True)\n",
    "df_processed['Fare_Rank'] = df_processed.groupby('Pclass')['Fare'].rank(pct=True)\n",
    "\n",
    "# 重要な交互作用\n",
    "df_processed['Sex_Pclass'] = df_processed['Sex_Binary'] * df_processed['Pclass']\n",
    "df_processed['Age_Sex'] = df_processed['Age'] * df_processed['Sex_Binary']\n",
    "df_processed['Fare_Pclass'] = df_processed['Fare'] / df_processed['Pclass']\n",
    "\n",
    "# 生存優先度スコア（ドメイン知識）\n",
    "df_processed['Priority_Score'] = (\n",
    "    (df_processed['Sex'] == 'female').astype(int) * 100 +\n",
    "    df_processed['Is_Child'] * 80 +\n",
    "    (df_processed['Pclass'] == 1).astype(int) * 30 +\n",
    "    df_processed['Is_Mother'] * 20\n",
    ")\n",
    "\n",
    "# 社会経済地位スコア\n",
    "df_processed['SES_Score'] = (\n",
    "    (4 - df_processed['Pclass']) * 25 +\n",
    "    df_processed['Fare_Rank'] * 100 +\n",
    "    df_processed['Has_Cabin'] * 50\n",
    ")\n",
    "\n",
    "print(\"✅ 高品質特徴量追加完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Phase 4: 特徴量選択（相関・重要度ベース）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ分割\n",
    "train_data = df_processed[df_processed['is_train']].copy()\n",
    "test_data = df_processed[~df_processed['is_train']].copy()\n",
    "\n",
    "# 特徴量候補\n",
    "exclude_cols = [\n",
    "    'PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived', 'is_train',\n",
    "    'Surname', 'Title', 'Sex', 'Embarked', 'Deck'\n",
    "]\n",
    "\n",
    "candidate_features = [col for col in df_processed.columns\n",
    "                     if col not in exclude_cols and\n",
    "                     df_processed[col].dtype in ['int64', 'float64', 'int32', 'float32', 'int8']]\n",
    "\n",
    "X_temp = train_data[candidate_features]\n",
    "y_temp = train_data['Survived']\n",
    "\n",
    "print(f\"特徴量候補: {len(candidate_features)}個\")\n",
    "\n",
    "# NaN処理\n",
    "for col in candidate_features:\n",
    "    if X_temp[col].isnull().any():\n",
    "        X_temp[col] = X_temp[col].fillna(X_temp[col].median())\n",
    "\n",
    "# 相関による特徴量除去\n",
    "corr_matrix = X_temp.corr().abs()\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    ")\n",
    "\n",
    "# 相関0.8以上の特徴量ペアを特定\n",
    "high_corr_pairs = [(col, row) for col in upper_triangle.columns\n",
    "                   for row in upper_triangle.index\n",
    "                   if upper_triangle.loc[row, col] > 0.8]\n",
    "\n",
    "print(f\"\\n高相関ペア ({len(high_corr_pairs)}組):\")\n",
    "for pair in high_corr_pairs:\n",
    "    corr_val = upper_triangle.loc[pair[1], pair[0]]\n",
    "    print(f\"  {pair[0]} - {pair[1]}: {corr_val:.3f}\")\n",
    "\n",
    "# 相関の高い特徴量を除去（重要度で決定）\n",
    "lgb_temp = lgb.LGBMClassifier(random_state=42, verbose=-1, n_estimators=100)\n",
    "lgb_temp.fit(X_temp, y_temp)\n",
    "importance_temp = pd.DataFrame({\n",
    "    'feature': candidate_features,\n",
    "    'importance': lgb_temp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# 相関の高いペアで重要度の低い方を除去\n",
    "features_to_remove = set()\n",
    "for pair in high_corr_pairs:\n",
    "    imp1 = importance_temp[importance_temp['feature'] == pair[0]]['importance'].values[0]\n",
    "    imp2 = importance_temp[importance_temp['feature'] == pair[1]]['importance'].values[0]\n",
    "    if imp1 < imp2:\n",
    "        features_to_remove.add(pair[0])\n",
    "    else:\n",
    "        features_to_remove.add(pair[1])\n",
    "\n",
    "# 重要度下位の特徴量も除去\n",
    "low_importance_features = importance_temp.tail(5)['feature'].tolist()\n",
    "features_to_remove.update(low_importance_features)\n",
    "\n",
    "# 最終特徴量セット\n",
    "final_features = [f for f in candidate_features if f not in features_to_remove]\n",
    "\n",
    "print(f\"\\n除去する特徴量 ({len(features_to_remove)}個): {sorted(features_to_remove)}\")\n",
    "print(f\"最終特徴量数: {len(final_features)}個\")\n",
    "\n",
    "# 最終データセット準備\n",
    "X_train_final = train_data[final_features].copy()\n",
    "y_train_final = train_data['Survived'].copy()\n",
    "X_test_final = test_data[final_features].copy()\n",
    "\n",
    "# 残りのNaN処理\n",
    "for col in final_features:\n",
    "    if X_train_final[col].isnull().any() or X_test_final[col].isnull().any():\n",
    "        median_val = X_train_final[col].median()\n",
    "        X_train_final[col] = X_train_final[col].fillna(median_val)\n",
    "        X_test_final[col] = X_test_final[col].fillna(median_val)\n",
    "\n",
    "print(\"\\n=== 最終特徴量リスト ===\")\n",
    "for i, feat in enumerate(final_features, 1):\n",
    "    imp_score = importance_temp[importance_temp['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"{i:2d}. {feat:25s}: {imp_score:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Phase 5: Base Modelsの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation設定\n",
    "N_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Out-of-fold予測用配列\n",
    "oof_predictions = np.zeros((len(X_train_final), 6))  # 6モデル\n",
    "test_predictions = np.zeros((len(X_test_final), 6))\n",
    "model_scores = {}\n",
    "\n",
    "print(\"🤖 Base Models構築開始...\")\n",
    "\n",
    "# ========== Model 1: LightGBM ==========\n",
    "print(\"\\n1. LightGBM訓練中...\")\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbose': -1,\n",
    "    'n_estimators': 500\n",
    "}\n",
    "\n",
    "lgb_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 0] = val_pred\n",
    "    test_predictions[:, 0] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    lgb_scores.append(fold_score)\n",
    "\n",
    "model_scores['LightGBM'] = np.mean(lgb_scores)\n",
    "print(f\"LightGBM CV: {np.mean(lgb_scores):.4f} ± {np.std(lgb_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== Model 2: XGBoost ==========\nprint(\"\\n2. XGBoost訓練中...\")\nxgb_scores = []\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n    \n    model = xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='logloss',\n        max_depth=4,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=RANDOM_SEED,\n        n_estimators=500,\n        verbosity=0\n    )\n    model.fit(X_tr, y_tr)\n    \n    val_pred = model.predict_proba(X_val)[:, 1]\n    test_pred = model.predict_proba(X_test_final)[:, 1]\n    \n    oof_predictions[val_idx, 1] = val_pred\n    test_predictions[:, 1] += test_pred / N_FOLDS\n    \n    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n    xgb_scores.append(fold_score)\n    \nmodel_scores['XGBoost'] = np.mean(xgb_scores)\nprint(f\"XGBoost CV: {np.mean(xgb_scores):.4f} ± {np.std(xgb_scores):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 3: Random Forest ==========\n",
    "print(\"\\n3. Random Forest訓練中...\")\n",
    "rf_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 2] = val_pred\n",
    "    test_predictions[:, 2] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    rf_scores.append(fold_score)\n",
    "\n",
    "model_scores['RandomForest'] = np.mean(rf_scores)\n",
    "print(f\"RandomForest CV: {np.mean(rf_scores):.4f} ± {np.std(rf_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 4: Extra Trees ==========\n",
    "print(\"\\n4. Extra Trees訓練中...\")\n",
    "et_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=False,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 3] = val_pred\n",
    "    test_predictions[:, 3] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    et_scores.append(fold_score)\n",
    "\n",
    "model_scores['ExtraTrees'] = np.mean(et_scores)\n",
    "print(f\"ExtraTrees CV: {np.mean(et_scores):.4f} ± {np.std(et_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 5: SVM ==========\n",
    "print(\"\\n5. SVM訓練中...\")\n",
    "# SVMのためのスケーリング\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "svm_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 4] = val_pred\n",
    "    test_predictions[:, 4] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    svm_scores.append(fold_score)\n",
    "\n",
    "model_scores['SVM'] = np.mean(svm_scores)\n",
    "print(f\"SVM CV: {np.mean(svm_scores):.4f} ± {np.std(svm_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 6: Logistic Regression ==========\n",
    "print(\"\\n6. Logistic Regression訓練中...\")\n",
    "lr_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=1.0,\n",
    "        penalty='l2',\n",
    "        solver='liblinear',\n",
    "        random_state=RANDOM_SEED,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 5] = val_pred\n",
    "    test_predictions[:, 5] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    lr_scores.append(fold_score)\n",
    "\n",
    "model_scores['LogisticRegression'] = np.mean(lr_scores)\n",
    "print(f\"LogisticRegression CV: {np.mean(lr_scores):.4f} ± {np.std(lr_scores):.4f}\")\n",
    "\n",
    "print(\"\\n✅ Base Models構築完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Phase 6: Stacking & Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Modelsの性能確認\n",
    "print(\"=== Base Models性能 ===\")\n",
    "model_names = ['LightGBM', 'XGBoost', 'RandomForest', 'ExtraTrees', 'SVM', 'LogisticRegression']\n",
    "for i, name in enumerate(model_names):\n",
    "    oof_score = accuracy_score(y_train_final, (oof_predictions[:, i] >= 0.5).astype(int))\n",
    "    print(f\"{name:15s}: {model_scores[name]:.4f} (OOF: {oof_score:.4f})\")\n",
    "\n",
    "# ========== Method 1: Simple Averaging ==========\n",
    "ensemble_avg = np.mean(oof_predictions, axis=1)\n",
    "ensemble_avg_score = accuracy_score(y_train_final, (ensemble_avg >= 0.5).astype(int))\n",
    "print(f\"\\nSimple Average: {ensemble_avg_score:.4f}\")\n",
    "\n",
    "# ========== Method 2: Weighted Averaging ==========\n",
    "# CVスコアベースの重み\n",
    "cv_scores = np.array([model_scores[name] for name in model_names])\n",
    "weights = cv_scores / np.sum(cv_scores)  # 正規化\n",
    "\n",
    "ensemble_weighted = np.average(oof_predictions, axis=1, weights=weights)\n",
    "ensemble_weighted_score = accuracy_score(y_train_final, (ensemble_weighted >= 0.5).astype(int))\n",
    "print(f\"Weighted Average: {ensemble_weighted_score:.4f}\")\n",
    "\n",
    "print(\"\\n重み分布:\")\n",
    "for i, (name, weight) in enumerate(zip(model_names, weights)):\n",
    "    print(f\"{name:15s}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== Method 3: Optuna Optimization ==========\nprint(\"\\n🔍 Optuna重み最適化開始...\")\n\ndef objective(trial):\n    # 各モデルの重みを0-1の範囲で最適化\n    w1 = trial.suggest_float('lgb_weight', 0.0, 1.0)\n    w2 = trial.suggest_float('xgb_weight', 0.0, 1.0)\n    w3 = trial.suggest_float('rf_weight', 0.0, 1.0)\n    w4 = trial.suggest_float('et_weight', 0.0, 1.0)\n    w5 = trial.suggest_float('svm_weight', 0.0, 1.0)\n    w6 = trial.suggest_float('lr_weight', 0.0, 1.0)\n    \n    weights = np.array([w1, w2, w3, w4, w5, w6])\n    weights = weights / np.sum(weights)  # 正規化\n    \n    # 重み付きアンサンブル\n    ensemble_pred = np.average(oof_predictions, axis=1, weights=weights)\n    ensemble_binary = (ensemble_pred >= 0.5).astype(int)\n    \n    return accuracy_score(y_train_final, ensemble_binary)\n\n# TODO(human): Optuna試行回数を設定 (100-500推奨)\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\nstudy.optimize(objective, n_trials=200, show_progress_bar=True)  # デフォルト200回\n\n# 最適重み\nbest_weights = np.array([\n    study.best_params['lgb_weight'],\n    study.best_params['xgb_weight'],\n    study.best_params['rf_weight'],\n    study.best_params['et_weight'],\n    study.best_params['svm_weight'],\n    study.best_params['lr_weight']\n])\nbest_weights = best_weights / np.sum(best_weights)\n\nensemble_optuna = np.average(oof_predictions, axis=1, weights=best_weights)\nensemble_optuna_score = accuracy_score(y_train_final, (ensemble_optuna >= 0.5).astype(int))\n\nprint(f\"\\nOptuna Optimized: {ensemble_optuna_score:.4f} (Best: {study.best_value:.4f})\")\nprint(\"\\n最適重み:\")\nfor name, weight in zip(model_names, best_weights):\n    print(f\"{name:15s}: {weight:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Method 4: Level-1 Stacking ==========\n",
    "print(\"\\n🏗️ Level-1 Stacking実装...\")\n",
    "\n",
    "# Meta-learnerとしてLightGBMを使用\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    num_leaves=15,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Level-1 Cross Validation\n",
    "stacking_scores = []\n",
    "stacking_oof = np.zeros(len(y_train_final))\n",
    "stacking_test = np.zeros(len(X_test_final))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(oof_predictions, y_train_final)):\n",
    "    X_meta_train = oof_predictions[train_idx]\n",
    "    X_meta_val = oof_predictions[val_idx]\n",
    "    y_meta_train = y_train_final.iloc[train_idx]\n",
    "    y_meta_val = y_train_final.iloc[val_idx]\n",
    "\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "    val_pred = meta_model.predict_proba(X_meta_val)[:, 1]\n",
    "    test_pred = meta_model.predict_proba(test_predictions)[:, 1]\n",
    "\n",
    "    stacking_oof[val_idx] = val_pred\n",
    "    stacking_test += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_meta_val, (val_pred >= 0.5).astype(int))\n",
    "    stacking_scores.append(fold_score)\n",
    "\n",
    "stacking_cv_score = np.mean(stacking_scores)\n",
    "stacking_oof_score = accuracy_score(y_train_final, (stacking_oof >= 0.5).astype(int))\n",
    "\n",
    "print(f\"Stacking CV: {stacking_cv_score:.4f} ± {np.std(stacking_scores):.4f}\")\n",
    "print(f\"Stacking OOF: {stacking_oof_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Phase 7: 最終結果と提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全アンサンブル手法の比較\n",
    "print(\"=== アンサンブル手法比較 ===\")\n",
    "print(f\"Simple Average:    {ensemble_avg_score:.4f}\")\n",
    "print(f\"Weighted Average:  {ensemble_weighted_score:.4f}\")\n",
    "print(f\"Optuna Optimized:  {ensemble_optuna_score:.4f}\")\n",
    "print(f\"Level-1 Stacking:  {stacking_oof_score:.4f}\")\n",
    "\n",
    "# 最高性能の手法を選択\n",
    "methods = {\n",
    "    'Simple Average': (ensemble_avg_score, np.mean(test_predictions, axis=1)),\n",
    "    'Weighted Average': (ensemble_weighted_score, np.average(test_predictions, axis=1, weights=weights)),\n",
    "    'Optuna Optimized': (ensemble_optuna_score, np.average(test_predictions, axis=1, weights=best_weights)),\n",
    "    'Level-1 Stacking': (stacking_oof_score, stacking_test)\n",
    "}\n",
    "\n",
    "best_method = max(methods.keys(), key=lambda k: methods[k][0])\n",
    "best_score = methods[best_method][0]\n",
    "best_predictions = methods[best_method][1]\n",
    "\n",
    "print(f\"\\n🏆 最高性能: {best_method} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過去実験との比較\n",
    "print(\"\\n=== 過去実験との比較 ===\")\n",
    "past_results = {\n",
    "    'exp004': 0.77990,\n",
    "    'exp007': 0.77751,\n",
    "    'exp008': 0.78468\n",
    "}\n",
    "\n",
    "print(\"過去のKaggleスコア:\")\n",
    "for exp, score in past_results.items():\n",
    "    print(f\"  {exp}: {score:.5f}\")\n",
    "\n",
    "print(f\"\\nexp009 予測CV: {best_score:.4f}\")\n",
    "\n",
    "# exp008との比較で期待スコア計算\n",
    "exp008_cv = 0.8541\n",
    "exp008_kaggle = 0.78468\n",
    "expected_ratio = exp008_kaggle / exp008_cv\n",
    "expected_kaggle = best_score * expected_ratio\n",
    "\n",
    "print(f\"期待Kaggleスコア: {expected_kaggle:.5f}\")\n",
    "if expected_kaggle > 0.83:\n",
    "    print(\"🎉 0.83突破の可能性大！\")\n",
    "elif expected_kaggle > past_results['exp008']:\n",
    "    improvement = expected_kaggle - past_results['exp008']\n",
    "    print(f\"✅ exp008から {improvement:+.5f} 改善期待\")\n",
    "else:\n",
    "    print(\"⚠️ さらなる改善が必要\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': (best_predictions >= 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# 予測分布確認\n",
    "print(\"=== 予測分布 ===\")\n",
    "print(f\"生存予測: {submission['Survived'].sum()} ({submission['Survived'].mean():.1%})\")\n",
    "print(f\"死亡予測: {len(submission) - submission['Survived'].sum()} ({1 - submission['Survived'].mean():.1%})\")\n",
    "print(f\"\\n訓練データ生存率: {y_train_final.mean():.1%}\")\n",
    "\n",
    "# ファイル保存\n",
    "import os\n",
    "os.makedirs('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp009', exist_ok=True)\n",
    "submission.to_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp009/result.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ 提出ファイル保存完了\")\n",
    "print(f\"Path: results/exp009/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終サマリー\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"         🚀 EXP009 PERFECT ENSEMBLE STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n🏆 最終結果:\")\n",
    "print(f\"  最高CV Score: {best_score:.4f} ({best_method})\")\n",
    "print(f\"  期待Kaggle: {expected_kaggle:.5f}\")\n",
    "print(f\"  使用特徴量: {len(final_features)}個\")\n",
    "\n",
    "print(f\"\\n🤖 Base Models性能:\")\n",
    "for name in model_names:\n",
    "    print(f\"  {name:15s}: {model_scores[name]:.4f}\")\n",
    "\n",
    "print(f\"\\n🏗️ アンサンブル手法:\")\n",
    "print(f\"  Simple Average:   {ensemble_avg_score:.4f}\")\n",
    "print(f\"  Weighted Average: {ensemble_weighted_score:.4f}\")\n",
    "print(f\"  Optuna Optimized: {ensemble_optuna_score:.4f}\")\n",
    "print(f\"  Level-1 Stacking: {stacking_oof_score:.4f}\")\n",
    "\n",
    "print(f\"\\n💡 技術的成果:\")\n",
    "print(f\"  • 6種類のBase Models構築\")\n",
    "print(f\"  • 4種類のアンサンブル手法実装\")\n",
    "print(f\"  • 高度な特徴量選択（相関・重要度ベース）\")\n",
    "print(f\"  • Optuna重み最適化\")\n",
    "print(f\"  • IterativeImputer欠損値補完\")\n",
    "print(f\"  • IQRベース外れ値処理\")\n",
    "\n",
    "if expected_kaggle > 0.83:\n",
    "    print(f\"\\n🎯 0.83の壁突破への道筋が見えた！\")\n",
    "    print(f\"期待改善: {expected_kaggle - past_results['exp008']:+.5f}\")\n",
    "    print(f\"Perfect Ensembleの威力を実感！\")\n",
    "else:\n",
    "    print(f\"\\n📈 着実な改善を達成！\")\n",
    "    if expected_kaggle > past_results['exp008']:\n",
    "        print(f\"exp008から {expected_kaggle - past_results['exp008']:+.5f} 改善\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  Perfect Ensemble - Many Models, One Goal!\")\n",
    "print(\"  0.83への挑戦結果をお待ちしています...\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}