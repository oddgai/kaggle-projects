{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp009 - Perfect Ensemble Strategy: 0.83ã®å£ã‚’çªç ´ã›ã‚ˆï¼\n",
    "\n",
    "## ğŸ¯ ç›®æ¨™\n",
    "- exp008ï¼ˆ0.78468ï¼‰ã‹ã‚‰å¤§å¹…æ”¹å–„\n",
    "- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§0.83ã‚’ç›®æŒ‡ã™\n",
    "- ç‰¹å¾´é‡æœ€é©åŒ– + Perfect Stacking + ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer  # ã“ã®è¡Œã‚’è¿½åŠ \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport optuna\nimport re\n\nplt.rcParams['font.family'] = 'IPAexGothic'\nprint(\"ğŸš€ exp009 - Perfect Ensemble Strategy\")\nprint(\"ç›®æ¨™: 0.83ã®å£ã‚’çªç ´ï¼\")\n\n# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\ntrain_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/train.csv')\ntest_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/test.csv')\n\n# ãƒ‡ãƒ¼ã‚¿çµåˆ\ndf_all = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\ndf_all['is_train'] = df_all['Survived'].notna()\n\nprint(f\"\\nãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:\")\nprint(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {df_all['is_train'].sum()}ä»¶\")\nprint(f\"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {(~df_all['is_train']).sum()}ä»¶\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Phase 1: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_features(df):\n",
    "    \"\"\"æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ========== Titleå‡¦ç† ==========\n",
    "    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "        'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss',\n",
    "        'Col': 'Officer', 'Major': 'Officer', 'Capt': 'Officer',\n",
    "        'Lady': 'Royalty', 'Sir': 'Royalty', 'Countess': 'Royalty',\n",
    "        'Don': 'Royalty', 'Dona': 'Royalty', 'Jonkheer': 'Royalty',\n",
    "        'Dr': 'Professional', 'Rev': 'Professional'\n",
    "    }\n",
    "    df['Title_Grouped'] = df['Title'].map(title_mapping).fillna('Other')\n",
    "\n",
    "    # ========== åŸºæœ¬å‰å‡¦ç† ==========\n",
    "    df['Sex_Binary'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "    # ========== Cabinå‡¦ç† ==========\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Deck'] = df['Cabin'].str[0]\n",
    "\n",
    "    deck_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1, 'T': 0}\n",
    "    df['Deck_Num'] = df['Deck'].map(deck_mapping)\n",
    "\n",
    "    # Pclassãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒƒã‚­æ¨å®š\n",
    "    df.loc[(df['Pclass'] == 1) & df['Deck'].isna(), 'Deck_Num'] = 5\n",
    "    df.loc[(df['Pclass'] == 2) & df['Deck'].isna(), 'Deck_Num'] = 3\n",
    "    df.loc[(df['Pclass'] == 3) & df['Deck'].isna(), 'Deck_Num'] = 1\n",
    "    df['Deck_Num'] = df['Deck_Num'].fillna(0)\n",
    "\n",
    "    # ========== Ticketå‡¦ç† ==========\n",
    "    ticket_counts = df['Ticket'].value_counts()\n",
    "    df['Ticket_Group_Size'] = df['Ticket'].map(ticket_counts)\n",
    "\n",
    "    df['Ticket_IsNumeric'] = df['Ticket'].str.isnumeric().astype(int)\n",
    "\n",
    "    # ========== è‹—å­—ãƒ»å®¶æ—é–¢ä¿‚ ==========\n",
    "    df['Surname'] = df['Name'].str.split(',').str[0]\n",
    "    surname_counts = df['Surname'].value_counts()\n",
    "    df['Surname_Count'] = df['Surname'].map(surname_counts)\n",
    "\n",
    "    # å®¶æ—ã‚¿ã‚¤ãƒ—\n",
    "    df['Is_Mother'] = ((df['Sex'] == 'female') & (df['Parch'] > 0) & (df['Age'] > 18)).astype(int)\n",
    "    df['Is_Child'] = ((df['Age'] <= 16) | (df['Title'] == 'Master')).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ç‰¹å¾´é‡ä½œæˆ\n",
    "print(\"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "df_processed = create_optimized_features(df_all)\n",
    "print(\"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Phase 2: é«˜åº¦ãªæ¬ æå€¤å‡¦ç†ãƒ»å¤–ã‚Œå€¤å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== Ageæ¬ æå€¤å‡¦ç†ï¼ˆIterativeImputerï¼‰ ==========\n# Ageäºˆæ¸¬ç”¨ç‰¹å¾´é‡\nage_features = ['Pclass', 'Sex_Binary', 'SibSp', 'Parch', 'Fare', 'Has_Cabin']\n\n# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\nle_title = LabelEncoder()\ndf_processed['Title_Encoded'] = le_title.fit_transform(df_processed['Title_Grouped'])\nage_features.append('Title_Encoded')\n\nle_embarked = LabelEncoder()\ndf_processed['Embarked'] = df_processed['Embarked'].fillna('S')  # æœ€é »å€¤è£œå®Œ\ndf_processed['Embarked_Encoded'] = le_embarked.fit_transform(df_processed['Embarked'])\nage_features.append('Embarked_Encoded')\n\n# Fareæ¬ æå€¤ã‚’å…ˆã«å‡¦ç†\ndf_processed['Fare'] = df_processed.groupby(['Pclass', 'Embarked'])['Fare'].transform(\n    lambda x: x.fillna(x.median())\n)\ndf_processed['Fare'] = df_processed['Fare'].fillna(df_processed['Fare'].median())\n\n# Ageäºˆæ¸¬\nprint(\"Ageæ¬ æå€¤ã‚’IterativeImputerã§è£œå®Œä¸­...\")\nage_imputer = IterativeImputer(random_state=42, max_iter=10)\nage_data = df_processed[age_features + ['Age']].copy()\nage_data_imputed = age_imputer.fit_transform(age_data)\ndf_processed['Age'] = age_data_imputed[:, -1]  # Ageåˆ—\n\nprint(f\"Ageè£œå®Œå®Œäº†: {df_processed['Age'].isnull().sum()}ä»¶ã®æ¬ æ\")\n\n# ========== å¤–ã‚Œå€¤å‡¦ç† ==========\n# Fareã®å¤–ã‚Œå€¤å‡¦ç†ï¼ˆIQRãƒ™ãƒ¼ã‚¹ï¼‰\ndef handle_outliers_iqr(df, column, factor=1.5):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - factor * IQR\n    upper_bound = Q3 + factor * IQR\n    \n    outliers_before = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n    print(f\"{column}ã®å¤–ã‚Œå€¤å‡¦ç†: {outliers_before}ä»¶ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°\")\n    return df\n\ndf_processed = handle_outliers_iqr(df_processed, 'Fare')\ndf_processed = handle_outliers_iqr(df_processed, 'Age')\n\nprint(\"âœ… æ¬ æå€¤ãƒ»å¤–ã‚Œå€¤å‡¦ç†å®Œäº†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Phase 3: é«˜å“è³ªç‰¹å¾´é‡ã®è¿½åŠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== é«˜å“è³ªç‰¹å¾´é‡ã®è¿½åŠ  ==========\n",
    "\n",
    "# é‹è³ƒã®æ­£è¦åŒ–\n",
    "df_processed['Fare_Per_Person'] = df_processed['Fare'] / df_processed['Ticket_Group_Size']\n",
    "\n",
    "# å¹´é½¢ãƒ»é‹è³ƒã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "df_processed['Age_Rank'] = df_processed.groupby(['Sex', 'Pclass'])['Age'].rank(pct=True)\n",
    "df_processed['Fare_Rank'] = df_processed.groupby('Pclass')['Fare'].rank(pct=True)\n",
    "\n",
    "# é‡è¦ãªäº¤äº’ä½œç”¨\n",
    "df_processed['Sex_Pclass'] = df_processed['Sex_Binary'] * df_processed['Pclass']\n",
    "df_processed['Age_Sex'] = df_processed['Age'] * df_processed['Sex_Binary']\n",
    "df_processed['Fare_Pclass'] = df_processed['Fare'] / df_processed['Pclass']\n",
    "\n",
    "# ç”Ÿå­˜å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼‰\n",
    "df_processed['Priority_Score'] = (\n",
    "    (df_processed['Sex'] == 'female').astype(int) * 100 +\n",
    "    df_processed['Is_Child'] * 80 +\n",
    "    (df_processed['Pclass'] == 1).astype(int) * 30 +\n",
    "    df_processed['Is_Mother'] * 20\n",
    ")\n",
    "\n",
    "# ç¤¾ä¼šçµŒæ¸ˆåœ°ä½ã‚¹ã‚³ã‚¢\n",
    "df_processed['SES_Score'] = (\n",
    "    (4 - df_processed['Pclass']) * 25 +\n",
    "    df_processed['Fare_Rank'] * 100 +\n",
    "    df_processed['Has_Cabin'] * 50\n",
    ")\n",
    "\n",
    "print(\"âœ… é«˜å“è³ªç‰¹å¾´é‡è¿½åŠ å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Phase 4: ç‰¹å¾´é‡é¸æŠï¼ˆç›¸é–¢ãƒ»é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "train_data = df_processed[df_processed['is_train']].copy()\n",
    "test_data = df_processed[~df_processed['is_train']].copy()\n",
    "\n",
    "# ç‰¹å¾´é‡å€™è£œ\n",
    "exclude_cols = [\n",
    "    'PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived', 'is_train',\n",
    "    'Surname', 'Title', 'Sex', 'Embarked', 'Deck'\n",
    "]\n",
    "\n",
    "candidate_features = [col for col in df_processed.columns\n",
    "                     if col not in exclude_cols and\n",
    "                     df_processed[col].dtype in ['int64', 'float64', 'int32', 'float32', 'int8']]\n",
    "\n",
    "X_temp = train_data[candidate_features]\n",
    "y_temp = train_data['Survived']\n",
    "\n",
    "print(f\"ç‰¹å¾´é‡å€™è£œ: {len(candidate_features)}å€‹\")\n",
    "\n",
    "# NaNå‡¦ç†\n",
    "for col in candidate_features:\n",
    "    if X_temp[col].isnull().any():\n",
    "        X_temp[col] = X_temp[col].fillna(X_temp[col].median())\n",
    "\n",
    "# ç›¸é–¢ã«ã‚ˆã‚‹ç‰¹å¾´é‡é™¤å»\n",
    "corr_matrix = X_temp.corr().abs()\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    ")\n",
    "\n",
    "# ç›¸é–¢0.8ä»¥ä¸Šã®ç‰¹å¾´é‡ãƒšã‚¢ã‚’ç‰¹å®š\n",
    "high_corr_pairs = [(col, row) for col in upper_triangle.columns\n",
    "                   for row in upper_triangle.index\n",
    "                   if upper_triangle.loc[row, col] > 0.8]\n",
    "\n",
    "print(f\"\\né«˜ç›¸é–¢ãƒšã‚¢ ({len(high_corr_pairs)}çµ„):\")\n",
    "for pair in high_corr_pairs:\n",
    "    corr_val = upper_triangle.loc[pair[1], pair[0]]\n",
    "    print(f\"  {pair[0]} - {pair[1]}: {corr_val:.3f}\")\n",
    "\n",
    "# ç›¸é–¢ã®é«˜ã„ç‰¹å¾´é‡ã‚’é™¤å»ï¼ˆé‡è¦åº¦ã§æ±ºå®šï¼‰\n",
    "lgb_temp = lgb.LGBMClassifier(random_state=42, verbose=-1, n_estimators=100)\n",
    "lgb_temp.fit(X_temp, y_temp)\n",
    "importance_temp = pd.DataFrame({\n",
    "    'feature': candidate_features,\n",
    "    'importance': lgb_temp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# ç›¸é–¢ã®é«˜ã„ãƒšã‚¢ã§é‡è¦åº¦ã®ä½ã„æ–¹ã‚’é™¤å»\n",
    "features_to_remove = set()\n",
    "for pair in high_corr_pairs:\n",
    "    imp1 = importance_temp[importance_temp['feature'] == pair[0]]['importance'].values[0]\n",
    "    imp2 = importance_temp[importance_temp['feature'] == pair[1]]['importance'].values[0]\n",
    "    if imp1 < imp2:\n",
    "        features_to_remove.add(pair[0])\n",
    "    else:\n",
    "        features_to_remove.add(pair[1])\n",
    "\n",
    "# é‡è¦åº¦ä¸‹ä½ã®ç‰¹å¾´é‡ã‚‚é™¤å»\n",
    "low_importance_features = importance_temp.tail(5)['feature'].tolist()\n",
    "features_to_remove.update(low_importance_features)\n",
    "\n",
    "# æœ€çµ‚ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ\n",
    "final_features = [f for f in candidate_features if f not in features_to_remove]\n",
    "\n",
    "print(f\"\\né™¤å»ã™ã‚‹ç‰¹å¾´é‡ ({len(features_to_remove)}å€‹): {sorted(features_to_remove)}\")\n",
    "print(f\"æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(final_features)}å€‹\")\n",
    "\n",
    "# æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™\n",
    "X_train_final = train_data[final_features].copy()\n",
    "y_train_final = train_data['Survived'].copy()\n",
    "X_test_final = test_data[final_features].copy()\n",
    "\n",
    "# æ®‹ã‚Šã®NaNå‡¦ç†\n",
    "for col in final_features:\n",
    "    if X_train_final[col].isnull().any() or X_test_final[col].isnull().any():\n",
    "        median_val = X_train_final[col].median()\n",
    "        X_train_final[col] = X_train_final[col].fillna(median_val)\n",
    "        X_test_final[col] = X_test_final[col].fillna(median_val)\n",
    "\n",
    "print(\"\\n=== æœ€çµ‚ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ ===\")\n",
    "for i, feat in enumerate(final_features, 1):\n",
    "    imp_score = importance_temp[importance_temp['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"{i:2d}. {feat:25s}: {imp_score:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Phase 5: Base Modelsã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validationè¨­å®š\n",
    "N_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Out-of-foldäºˆæ¸¬ç”¨é…åˆ—\n",
    "oof_predictions = np.zeros((len(X_train_final), 6))  # 6ãƒ¢ãƒ‡ãƒ«\n",
    "test_predictions = np.zeros((len(X_test_final), 6))\n",
    "model_scores = {}\n",
    "\n",
    "print(\"ğŸ¤– Base Modelsæ§‹ç¯‰é–‹å§‹...\")\n",
    "\n",
    "# ========== Model 1: LightGBM ==========\n",
    "print(\"\\n1. LightGBMè¨“ç·´ä¸­...\")\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbose': -1,\n",
    "    'n_estimators': 500\n",
    "}\n",
    "\n",
    "lgb_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 0] = val_pred\n",
    "    test_predictions[:, 0] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    lgb_scores.append(fold_score)\n",
    "\n",
    "model_scores['LightGBM'] = np.mean(lgb_scores)\n",
    "print(f\"LightGBM CV: {np.mean(lgb_scores):.4f} Â± {np.std(lgb_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== Model 2: XGBoost ==========\nprint(\"\\n2. XGBoostè¨“ç·´ä¸­...\")\nxgb_scores = []\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n    \n    model = xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='logloss',\n        max_depth=4,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=RANDOM_SEED,\n        n_estimators=500,\n        verbosity=0\n    )\n    model.fit(X_tr, y_tr)\n    \n    val_pred = model.predict_proba(X_val)[:, 1]\n    test_pred = model.predict_proba(X_test_final)[:, 1]\n    \n    oof_predictions[val_idx, 1] = val_pred\n    test_predictions[:, 1] += test_pred / N_FOLDS\n    \n    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n    xgb_scores.append(fold_score)\n    \nmodel_scores['XGBoost'] = np.mean(xgb_scores)\nprint(f\"XGBoost CV: {np.mean(xgb_scores):.4f} Â± {np.std(xgb_scores):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 3: Random Forest ==========\n",
    "print(\"\\n3. Random Forestè¨“ç·´ä¸­...\")\n",
    "rf_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 2] = val_pred\n",
    "    test_predictions[:, 2] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    rf_scores.append(fold_score)\n",
    "\n",
    "model_scores['RandomForest'] = np.mean(rf_scores)\n",
    "print(f\"RandomForest CV: {np.mean(rf_scores):.4f} Â± {np.std(rf_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 4: Extra Trees ==========\n",
    "print(\"\\n4. Extra Treesè¨“ç·´ä¸­...\")\n",
    "et_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=False,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 3] = val_pred\n",
    "    test_predictions[:, 3] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    et_scores.append(fold_score)\n",
    "\n",
    "model_scores['ExtraTrees'] = np.mean(et_scores)\n",
    "print(f\"ExtraTrees CV: {np.mean(et_scores):.4f} Â± {np.std(et_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 5: SVM ==========\n",
    "print(\"\\n5. SVMè¨“ç·´ä¸­...\")\n",
    "# SVMã®ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "svm_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 4] = val_pred\n",
    "    test_predictions[:, 4] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    svm_scores.append(fold_score)\n",
    "\n",
    "model_scores['SVM'] = np.mean(svm_scores)\n",
    "print(f\"SVM CV: {np.mean(svm_scores):.4f} Â± {np.std(svm_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model 6: Logistic Regression ==========\n",
    "print(\"\\n6. Logistic Regressionè¨“ç·´ä¸­...\")\n",
    "lr_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final, y_train_final)):\n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_final.iloc[train_idx], y_train_final.iloc[val_idx]\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=1.0,\n",
    "        penalty='l2',\n",
    "        solver='liblinear',\n",
    "        random_state=RANDOM_SEED,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    oof_predictions[val_idx, 5] = val_pred\n",
    "    test_predictions[:, 5] += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    lr_scores.append(fold_score)\n",
    "\n",
    "model_scores['LogisticRegression'] = np.mean(lr_scores)\n",
    "print(f\"LogisticRegression CV: {np.mean(lr_scores):.4f} Â± {np.std(lr_scores):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Base Modelsæ§‹ç¯‰å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Phase 6: Stacking & Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Modelsã®æ€§èƒ½ç¢ºèª\n",
    "print(\"=== Base Modelsæ€§èƒ½ ===\")\n",
    "model_names = ['LightGBM', 'XGBoost', 'RandomForest', 'ExtraTrees', 'SVM', 'LogisticRegression']\n",
    "for i, name in enumerate(model_names):\n",
    "    oof_score = accuracy_score(y_train_final, (oof_predictions[:, i] >= 0.5).astype(int))\n",
    "    print(f\"{name:15s}: {model_scores[name]:.4f} (OOF: {oof_score:.4f})\")\n",
    "\n",
    "# ========== Method 1: Simple Averaging ==========\n",
    "ensemble_avg = np.mean(oof_predictions, axis=1)\n",
    "ensemble_avg_score = accuracy_score(y_train_final, (ensemble_avg >= 0.5).astype(int))\n",
    "print(f\"\\nSimple Average: {ensemble_avg_score:.4f}\")\n",
    "\n",
    "# ========== Method 2: Weighted Averaging ==========\n",
    "# CVã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®é‡ã¿\n",
    "cv_scores = np.array([model_scores[name] for name in model_names])\n",
    "weights = cv_scores / np.sum(cv_scores)  # æ­£è¦åŒ–\n",
    "\n",
    "ensemble_weighted = np.average(oof_predictions, axis=1, weights=weights)\n",
    "ensemble_weighted_score = accuracy_score(y_train_final, (ensemble_weighted >= 0.5).astype(int))\n",
    "print(f\"Weighted Average: {ensemble_weighted_score:.4f}\")\n",
    "\n",
    "print(\"\\né‡ã¿åˆ†å¸ƒ:\")\n",
    "for i, (name, weight) in enumerate(zip(model_names, weights)):\n",
    "    print(f\"{name:15s}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== Method 3: Optuna Optimization ==========\nprint(\"\\nğŸ” Optunaé‡ã¿æœ€é©åŒ–é–‹å§‹...\")\n\ndef objective(trial):\n    # å„ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’0-1ã®ç¯„å›²ã§æœ€é©åŒ–\n    w1 = trial.suggest_float('lgb_weight', 0.0, 1.0)\n    w2 = trial.suggest_float('xgb_weight', 0.0, 1.0)\n    w3 = trial.suggest_float('rf_weight', 0.0, 1.0)\n    w4 = trial.suggest_float('et_weight', 0.0, 1.0)\n    w5 = trial.suggest_float('svm_weight', 0.0, 1.0)\n    w6 = trial.suggest_float('lr_weight', 0.0, 1.0)\n    \n    weights = np.array([w1, w2, w3, w4, w5, w6])\n    weights = weights / np.sum(weights)  # æ­£è¦åŒ–\n    \n    # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n    ensemble_pred = np.average(oof_predictions, axis=1, weights=weights)\n    ensemble_binary = (ensemble_pred >= 0.5).astype(int)\n    \n    return accuracy_score(y_train_final, ensemble_binary)\n\n# TODO(human): Optunaè©¦è¡Œå›æ•°ã‚’è¨­å®š (100-500æ¨å¥¨)\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\nstudy.optimize(objective, n_trials=200, show_progress_bar=True)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ200å›\n\n# æœ€é©é‡ã¿\nbest_weights = np.array([\n    study.best_params['lgb_weight'],\n    study.best_params['xgb_weight'],\n    study.best_params['rf_weight'],\n    study.best_params['et_weight'],\n    study.best_params['svm_weight'],\n    study.best_params['lr_weight']\n])\nbest_weights = best_weights / np.sum(best_weights)\n\nensemble_optuna = np.average(oof_predictions, axis=1, weights=best_weights)\nensemble_optuna_score = accuracy_score(y_train_final, (ensemble_optuna >= 0.5).astype(int))\n\nprint(f\"\\nOptuna Optimized: {ensemble_optuna_score:.4f} (Best: {study.best_value:.4f})\")\nprint(\"\\næœ€é©é‡ã¿:\")\nfor name, weight in zip(model_names, best_weights):\n    print(f\"{name:15s}: {weight:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Method 4: Level-1 Stacking ==========\n",
    "print(\"\\nğŸ—ï¸ Level-1 Stackingå®Ÿè£…...\")\n",
    "\n",
    "# Meta-learnerã¨ã—ã¦LightGBMã‚’ä½¿ç”¨\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    num_leaves=15,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Level-1 Cross Validation\n",
    "stacking_scores = []\n",
    "stacking_oof = np.zeros(len(y_train_final))\n",
    "stacking_test = np.zeros(len(X_test_final))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(oof_predictions, y_train_final)):\n",
    "    X_meta_train = oof_predictions[train_idx]\n",
    "    X_meta_val = oof_predictions[val_idx]\n",
    "    y_meta_train = y_train_final.iloc[train_idx]\n",
    "    y_meta_val = y_train_final.iloc[val_idx]\n",
    "\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "    val_pred = meta_model.predict_proba(X_meta_val)[:, 1]\n",
    "    test_pred = meta_model.predict_proba(test_predictions)[:, 1]\n",
    "\n",
    "    stacking_oof[val_idx] = val_pred\n",
    "    stacking_test += test_pred / N_FOLDS\n",
    "\n",
    "    fold_score = accuracy_score(y_meta_val, (val_pred >= 0.5).astype(int))\n",
    "    stacking_scores.append(fold_score)\n",
    "\n",
    "stacking_cv_score = np.mean(stacking_scores)\n",
    "stacking_oof_score = accuracy_score(y_train_final, (stacking_oof >= 0.5).astype(int))\n",
    "\n",
    "print(f\"Stacking CV: {stacking_cv_score:.4f} Â± {np.std(stacking_scores):.4f}\")\n",
    "print(f\"Stacking OOF: {stacking_oof_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Phase 7: æœ€çµ‚çµæœã¨æå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¯”è¼ƒ\n",
    "print(\"=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•æ¯”è¼ƒ ===\")\n",
    "print(f\"Simple Average:    {ensemble_avg_score:.4f}\")\n",
    "print(f\"Weighted Average:  {ensemble_weighted_score:.4f}\")\n",
    "print(f\"Optuna Optimized:  {ensemble_optuna_score:.4f}\")\n",
    "print(f\"Level-1 Stacking:  {stacking_oof_score:.4f}\")\n",
    "\n",
    "# æœ€é«˜æ€§èƒ½ã®æ‰‹æ³•ã‚’é¸æŠ\n",
    "methods = {\n",
    "    'Simple Average': (ensemble_avg_score, np.mean(test_predictions, axis=1)),\n",
    "    'Weighted Average': (ensemble_weighted_score, np.average(test_predictions, axis=1, weights=weights)),\n",
    "    'Optuna Optimized': (ensemble_optuna_score, np.average(test_predictions, axis=1, weights=best_weights)),\n",
    "    'Level-1 Stacking': (stacking_oof_score, stacking_test)\n",
    "}\n",
    "\n",
    "best_method = max(methods.keys(), key=lambda k: methods[k][0])\n",
    "best_score = methods[best_method][0]\n",
    "best_predictions = methods[best_method][1]\n",
    "\n",
    "print(f\"\\nğŸ† æœ€é«˜æ€§èƒ½: {best_method} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éå»å®Ÿé¨“ã¨ã®æ¯”è¼ƒ\n",
    "print(\"\\n=== éå»å®Ÿé¨“ã¨ã®æ¯”è¼ƒ ===\")\n",
    "past_results = {\n",
    "    'exp004': 0.77990,\n",
    "    'exp007': 0.77751,\n",
    "    'exp008': 0.78468\n",
    "}\n",
    "\n",
    "print(\"éå»ã®Kaggleã‚¹ã‚³ã‚¢:\")\n",
    "for exp, score in past_results.items():\n",
    "    print(f\"  {exp}: {score:.5f}\")\n",
    "\n",
    "print(f\"\\nexp009 äºˆæ¸¬CV: {best_score:.4f}\")\n",
    "\n",
    "# exp008ã¨ã®æ¯”è¼ƒã§æœŸå¾…ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "exp008_cv = 0.8541\n",
    "exp008_kaggle = 0.78468\n",
    "expected_ratio = exp008_kaggle / exp008_cv\n",
    "expected_kaggle = best_score * expected_ratio\n",
    "\n",
    "print(f\"æœŸå¾…Kaggleã‚¹ã‚³ã‚¢: {expected_kaggle:.5f}\")\n",
    "if expected_kaggle > 0.83:\n",
    "    print(\"ğŸ‰ 0.83çªç ´ã®å¯èƒ½æ€§å¤§ï¼\")\n",
    "elif expected_kaggle > past_results['exp008']:\n",
    "    improvement = expected_kaggle - past_results['exp008']\n",
    "    print(f\"âœ… exp008ã‹ã‚‰ {improvement:+.5f} æ”¹å–„æœŸå¾…\")\n",
    "else:\n",
    "    print(\"âš ï¸ ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': (best_predictions >= 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# äºˆæ¸¬åˆ†å¸ƒç¢ºèª\n",
    "print(\"=== äºˆæ¸¬åˆ†å¸ƒ ===\")\n",
    "print(f\"ç”Ÿå­˜äºˆæ¸¬: {submission['Survived'].sum()} ({submission['Survived'].mean():.1%})\")\n",
    "print(f\"æ­»äº¡äºˆæ¸¬: {len(submission) - submission['Survived'].sum()} ({1 - submission['Survived'].mean():.1%})\")\n",
    "print(f\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿå­˜ç‡: {y_train_final.mean():.1%}\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "import os\n",
    "os.makedirs('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp009', exist_ok=True)\n",
    "submission.to_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp009/result.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†\")\n",
    "print(f\"Path: results/exp009/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚ã‚µãƒãƒªãƒ¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"         ğŸš€ EXP009 PERFECT ENSEMBLE STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ† æœ€çµ‚çµæœ:\")\n",
    "print(f\"  æœ€é«˜CV Score: {best_score:.4f} ({best_method})\")\n",
    "print(f\"  æœŸå¾…Kaggle: {expected_kaggle:.5f}\")\n",
    "print(f\"  ä½¿ç”¨ç‰¹å¾´é‡: {len(final_features)}å€‹\")\n",
    "\n",
    "print(f\"\\nğŸ¤– Base Modelsæ€§èƒ½:\")\n",
    "for name in model_names:\n",
    "    print(f\"  {name:15s}: {model_scores[name]:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•:\")\n",
    "print(f\"  Simple Average:   {ensemble_avg_score:.4f}\")\n",
    "print(f\"  Weighted Average: {ensemble_weighted_score:.4f}\")\n",
    "print(f\"  Optuna Optimized: {ensemble_optuna_score:.4f}\")\n",
    "print(f\"  Level-1 Stacking: {stacking_oof_score:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ æŠ€è¡“çš„æˆæœ:\")\n",
    "print(f\"  â€¢ 6ç¨®é¡ã®Base Modelsæ§‹ç¯‰\")\n",
    "print(f\"  â€¢ 4ç¨®é¡ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…\")\n",
    "print(f\"  â€¢ é«˜åº¦ãªç‰¹å¾´é‡é¸æŠï¼ˆç›¸é–¢ãƒ»é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ï¼‰\")\n",
    "print(f\"  â€¢ Optunaé‡ã¿æœ€é©åŒ–\")\n",
    "print(f\"  â€¢ IterativeImputeræ¬ æå€¤è£œå®Œ\")\n",
    "print(f\"  â€¢ IQRãƒ™ãƒ¼ã‚¹å¤–ã‚Œå€¤å‡¦ç†\")\n",
    "\n",
    "if expected_kaggle > 0.83:\n",
    "    print(f\"\\nğŸ¯ 0.83ã®å£çªç ´ã¸ã®é“ç­‹ãŒè¦‹ãˆãŸï¼\")\n",
    "    print(f\"æœŸå¾…æ”¹å–„: {expected_kaggle - past_results['exp008']:+.5f}\")\n",
    "    print(f\"Perfect Ensembleã®å¨åŠ›ã‚’å®Ÿæ„Ÿï¼\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“ˆ ç€å®Ÿãªæ”¹å–„ã‚’é”æˆï¼\")\n",
    "    if expected_kaggle > past_results['exp008']:\n",
    "        print(f\"exp008ã‹ã‚‰ {expected_kaggle - past_results['exp008']:+.5f} æ”¹å–„\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  Perfect Ensemble - Many Models, One Goal!\")\n",
    "print(\"  0.83ã¸ã®æŒ‘æˆ¦çµæœã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™...\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}