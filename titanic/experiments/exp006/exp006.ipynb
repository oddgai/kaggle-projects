{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp006 - 「Less is More」による精密最適化\n",
    "\n",
    "exp005の失敗（過学習）を受けて、シンプル化と精密最適化による堅実な改善を目指す\n",
    "\n",
    "## 戦略\n",
    "1. **特徴量削減**: 45個 → 15-18個\n",
    "2. **単一モデル集中**: LightGBMのみ\n",
    "3. **ハイパーパラメータ最適化**: Optuna活用\n",
    "4. **汎化性能重視**: CV vs Kaggle乖離監視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "\n",
    "print(\"🎯 exp006 - Less is More戦略開始\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"Optuna: {optuna.__version__}\")\n",
    "\n",
    "# データ読み込み\n",
    "train_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/train.csv')\n",
    "test_df = pd.read_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/data/test.csv')\n",
    "\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 準備・分析（exp004特徴量重要度分析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp004と同じ特徴量エンジニアリング（比較のため）\n",
    "def exp004_feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 名前からの特徴量\n",
    "    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.')\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
    "        'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Mrs', 'Lady': 'Rare',\n",
    "        'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
    "        'Capt': 'Rare', 'Sir': 'Rare'\n",
    "    }\n",
    "    df['Title_Grouped'] = df['Title'].map(title_mapping).fillna('Other')\n",
    "    df['Name_Length'] = df['Name'].str.len()\n",
    "    \n",
    "    # 苗字と家族サイズ\n",
    "    df['Surname'] = df['Name'].str.split(',').str[0]\n",
    "    all_surnames = pd.concat([train_df['Name'], test_df['Name']]).str.split(',').str[0]\n",
    "    surname_counts = all_surnames.value_counts()\n",
    "    df['Surname_Count'] = df['Surname'].map(surname_counts)\n",
    "    \n",
    "    # チケット特徴量\n",
    "    df['Ticket_Length'] = df['Ticket'].str.len()\n",
    "    df['Ticket_IsNumeric'] = df['Ticket'].str.isnumeric().astype(int)\n",
    "    df['Ticket_Prefix'] = df['Ticket'].str.extract(r'^([A-Za-z]+)').fillna('NUMERIC')\n",
    "    \n",
    "    all_tickets = pd.concat([train_df['Ticket'], test_df['Ticket']])\n",
    "    ticket_counts = all_tickets.value_counts()\n",
    "    df['Ticket_Count'] = df['Ticket'].map(ticket_counts)\n",
    "    \n",
    "    # 客室特徴量\n",
    "    df['HasCabin'] = (~df['Cabin'].isnull()).astype(int)\n",
    "    df['Cabin_Deck'] = df['Cabin'].str.extract(r'^([A-Za-z])').fillna('Unknown')\n",
    "    df['Cabin_Number'] = df['Cabin'].str.extract(r'(\\d+)').astype(float)\n",
    "    df['Cabin_Count'] = df['Cabin'].fillna('').str.split().str.len()\n",
    "    df.loc[df['Cabin'].isnull(), 'Cabin_Count'] = 0\n",
    "    \n",
    "    # 基本前処理\n",
    "    df['Sex_Binary'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "    df['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Fare'] = df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # 家族構成\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    df['IsSmallFamily'] = ((df['FamilySize'] >= 2) & (df['FamilySize'] <= 4)).astype(int)\n",
    "    df['IsLargeFamily'] = (df['FamilySize'] > 4).astype(int)\n",
    "    \n",
    "    # 年齢・運賃グループ\n",
    "    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 12, 18, 25, 35, 50, 65, 100], \n",
    "                           labels=['Child', 'Teen', 'Young', 'Adult', 'Middle', 'Senior', 'Elder'])\n",
    "    df['Fare_Group'] = pd.qcut(df['Fare'], q=8, labels=['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8'])\n",
    "    \n",
    "    # 基本交互作用\n",
    "    df['Sex_Pclass'] = df['Sex_Binary'] * df['Pclass']\n",
    "    df['Age_Fare_Interaction'] = df['Age'] * df['Fare']\n",
    "    df['Age_FamilySize'] = df['Age'] * df['FamilySize']\n",
    "    \n",
    "    # 統計特徴量\n",
    "    df['Age_Rank_SexPclass'] = df.groupby(['Sex_Binary', 'Pclass'])['Age'].rank(pct=True)\n",
    "    df['Fare_Rank_Pclass'] = df.groupby('Pclass')['Fare'].rank(pct=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# exp004特徴量でベースライン確認\n",
    "print(\"=== Step 1: exp004特徴量重要度分析 ===\")\n",
    "train_processed = exp004_feature_engineering(train_df)\n",
    "test_processed = exp004_feature_engineering(test_df)\n",
    "\n",
    "print(f\"exp004特徴量数: {train_processed.shape[1] - train_df.shape[1]}個（追加分）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカル特徴量のエンコーディング\n",
    "categorical_features = ['Embarked', 'Title_Grouped', 'Cabin_Deck', 'Ticket_Prefix', 'Age_Group', 'Fare_Group']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in train_processed.columns:\n",
    "        le = LabelEncoder()\n",
    "        all_categories = pd.concat([train_processed[feature], test_processed[feature]]).astype(str)\n",
    "        le.fit(all_categories)\n",
    "        \n",
    "        train_processed[feature] = le.transform(train_processed[feature].astype(str))\n",
    "        test_processed[feature] = le.transform(test_processed[feature].astype(str))\n",
    "\n",
    "# exp004相当の特徴量選択\n",
    "exclude_features = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived', 'Surname', 'Title', 'Sex']\n",
    "exp004_features = [col for col in train_processed.columns \n",
    "                  if col not in exclude_features and \n",
    "                  train_processed[col].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
    "\n",
    "X_full = train_processed[exp004_features]\n",
    "y = train_processed['Survived']\n",
    "X_test_full = test_processed[exp004_features]\n",
    "\n",
    "print(f\"exp004相当特徴量数: {len(exp004_features)}\")\n",
    "print(\"\\nexp004特徴量一覧:\")\n",
    "for i, feat in enumerate(exp004_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "# 重要度取得のため簡易モデル訓練\n",
    "print(\"\\n特徴量重要度分析のため簡易LightGBM訓練...\")\n",
    "lgb_params_simple = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "feature_importance_scores = np.zeros(len(exp004_features))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full, y)):\n",
    "    X_train, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params_simple,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    feature_importance_scores += model.feature_importance()\n",
    "\n",
    "# 平均重要度計算\n",
    "feature_importance_scores /= 5\n",
    "\n",
    "# 重要度データフレーム作成\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': exp004_features,\n",
    "    'importance': feature_importance_scores\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== exp004特徴量重要度ランキング ===\")\n",
    "for i, row in importance_df.iterrows():\n",
    "    print(f\"{importance_df.index.get_loc(i)+1:2d}. {row['feature']:20s}: {row['importance']:8.1f}\")\n",
    "\n",
    "# 重要度可視化\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('重要度')\n",
    "plt.title('exp004特徴量重要度 Top 15')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n重要度分析完了。次のステップで削減候補を決定します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 段階的特徴量削減実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step 2: 段階的特徴量削減実験 ===\")\n",
    "\n",
    "# 削減戦略の設定\n",
    "def create_feature_sets(importance_df, exp004_features):\n",
    "    \"\"\"\n",
    "    重要度に基づいて段階的な特徴量セットを作成\n",
    "    \"\"\"\n",
    "    # 基本特徴量（絶対に残す）\n",
    "    core_features = ['Sex_Binary', 'Pclass', 'Age', 'Fare']\n",
    "    \n",
    "    # 重要度順にソート済みの特徴量リスト\n",
    "    sorted_features = importance_df['feature'].tolist()\n",
    "    \n",
    "    # 各段階の特徴量セット定義\n",
    "    feature_sets = {\n",
    "        '20features': {\n",
    "            'features': sorted_features[:20],\n",
    "            'description': '重要度上位20特徴量'\n",
    "        },\n",
    "        '18features': {\n",
    "            'features': sorted_features[:18], \n",
    "            'description': '重要度上位18特徴量'\n",
    "        },\n",
    "        '15features': {\n",
    "            'features': sorted_features[:15],\n",
    "            'description': '重要度上位15特徴量'\n",
    "        },\n",
    "        '12features': {\n",
    "            'features': sorted_features[:12],\n",
    "            'description': '重要度上位12特徴量（攻めの削減）'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # コア特徴量が含まれていることを確認\n",
    "    for set_name, set_info in feature_sets.items():\n",
    "        missing_core = [f for f in core_features if f not in set_info['features']]\n",
    "        if missing_core:\n",
    "            print(f\"⚠️ {set_name}にコア特徴量{missing_core}が不足\")\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "# 特徴量セット作成\n",
    "feature_sets = create_feature_sets(importance_df, exp004_features)\n",
    "\n",
    "# 各セットの内容表示\n",
    "for set_name, set_info in feature_sets.items():\n",
    "    print(f\"\\n{set_name} ({set_info['description']}):\")\n",
    "    for i, feat in enumerate(set_info['features'], 1):\n",
    "        importance = importance_df[importance_df['feature'] == feat]['importance'].iloc[0]\n",
    "        print(f\"{i:2d}. {feat:20s} ({importance:6.1f})\")\n",
    "\n",
    "print(\"\\n特徴量セット定義完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各特徴量セットでの性能評価\n",
    "def evaluate_feature_set(X, y, features, set_name, n_folds=5):\n",
    "    \"\"\"\n",
    "    指定された特徴量セットでCV性能を評価\n",
    "    \"\"\"\n",
    "    X_subset = X[features]\n",
    "    \n",
    "    # LightGBMパラメータ（exp004準拠）\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    early_stop_rounds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_subset, y)):\n",
    "        X_train, X_val = X_subset.iloc[train_idx], X_subset.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            lgb_params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        val_pred = model.predict(X_val)\n",
    "        val_pred_binary = (val_pred >= 0.5).astype(int)\n",
    "        fold_score = accuracy_score(y_val, val_pred_binary)\n",
    "        cv_scores.append(fold_score)\n",
    "        early_stop_rounds.append(model.best_iteration)\n",
    "    \n",
    "    mean_cv = np.mean(cv_scores)\n",
    "    std_cv = np.std(cv_scores)\n",
    "    mean_early_stop = np.mean(early_stop_rounds)\n",
    "    \n",
    "    return {\n",
    "        'set_name': set_name,\n",
    "        'n_features': len(features),\n",
    "        'cv_mean': mean_cv,\n",
    "        'cv_std': std_cv,\n",
    "        'early_stop_mean': mean_early_stop,\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "\n",
    "# 全特徴量セットで評価実行\n",
    "print(\"各特徴量セットでの性能評価開始...\")\n",
    "\n",
    "evaluation_results = []\n",
    "for set_name, set_info in feature_sets.items():\n",
    "    print(f\"\\n📊 {set_name} 評価中...\")\n",
    "    result = evaluate_feature_set(X_full, y, set_info['features'], set_name)\n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"CV: {result['cv_mean']:.4f} ± {result['cv_std']:.4f}\")\n",
    "    print(f\"Early Stop平均: {result['early_stop_mean']:.1f}ラウンド\")\n",
    "\n",
    "# 結果比較\n",
    "print(\"\\n=== 特徴量削減結果比較 ===\")\n",
    "print(f\"{'Set':15s} {'Features':8s} {'CV Mean':8s} {'CV Std':8s} {'Early Stop':10s} {'期待Kaggle':10s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# exp004基準の期待Kaggle計算（経験式）\n",
    "exp004_cv = 0.8462\n",
    "exp004_kaggle = 0.77990\n",
    "cv_kaggle_ratio = exp004_kaggle / exp004_cv\n",
    "\n",
    "best_result = None\n",
    "best_score = 0\n",
    "\n",
    "for result in evaluation_results:\n",
    "    expected_kaggle = result['cv_mean'] * cv_kaggle_ratio\n",
    "    print(f\"{result['set_name']:15s} {result['n_features']:8d} {result['cv_mean']:8.4f} {result['cv_std']:8.4f} {result['early_stop_mean']:10.1f} {expected_kaggle:10.5f}\")\n",
    "    \n",
    "    # 期待Kaggleが最高のものを選択\n",
    "    if expected_kaggle > best_score:\n",
    "        best_score = expected_kaggle\n",
    "        best_result = result\n",
    "\n",
    "print(f\"\\n🏆 最高性能特徴量セット: {best_result['set_name']}\")\n",
    "print(f\"📊 CV性能: {best_result['cv_mean']:.4f} ± {best_result['cv_std']:.4f}\")\n",
    "print(f\"🎯 期待Kaggleスコア: {best_score:.5f}\")\n",
    "\n",
    "# 最適特徴量セット決定\n",
    "optimal_features = feature_sets[best_result['set_name']]['features']\n",
    "print(f\"\\n最適特徴量数: {len(optimal_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ハイパーパラメータ最適化（Optuna）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step 3: ハイパーパラメータ最適化 ===\")\n",
    "print(f\"最適特徴量セット: {best_result['set_name']} ({len(optimal_features)}特徴量)\")\n",
    "\n",
    "# 最適化用データ準備\n",
    "X_optimal = X_full[optimal_features]\n",
    "X_test_optimal = X_test_full[optimal_features]\n",
    "\n",
    "print(\"\\n使用する特徴量:\")\n",
    "for i, feat in enumerate(optimal_features, 1):\n",
    "    importance = importance_df[importance_df['feature'] == feat]['importance'].iloc[0]\n",
    "    print(f\"{i:2d}. {feat:20s} ({importance:6.1f})\")\n",
    "\n",
    "# Optuna最適化関数定義\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optunaの目的関数：CV精度を最大化\n",
    "    \"\"\"\n",
    "    # ハイパーパラメータ探索空間\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        \n",
    "        # 主要パラメータ\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 60),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        \n",
    "        # 正則化パラメータ（重点的に探索）\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 1.0)\n",
    "    }\n",
    "    \n",
    "    # 5-fold交差検証\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    early_stop_counts = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_optimal, y):\n",
    "        X_train, X_val = X_optimal.iloc[train_idx], X_optimal.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        val_pred = model.predict(X_val)\n",
    "        val_pred_binary = (val_pred >= 0.5).astype(int)\n",
    "        fold_score = accuracy_score(y_val, val_pred_binary)\n",
    "        cv_scores.append(fold_score)\n",
    "        early_stop_counts.append(model.best_iteration)\n",
    "    \n",
    "    mean_cv = np.mean(cv_scores)\n",
    "    mean_early_stop = np.mean(early_stop_counts)\n",
    "    \n",
    "    # 過学習の兆候がある場合ペナルティ\n",
    "    if mean_early_stop > 800:  # 早期停止しない場合\n",
    "        mean_cv *= 0.99  # 軽いペナルティ\n",
    "    \n",
    "    return mean_cv\n",
    "\n",
    "# Optuna最適化実行\n",
    "print(\"\\nOptuna最適化開始（100回試行）...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "# 最適化実行\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n=== 最適化結果 ===\")\n",
    "print(f\"最高CV精度: {study.best_value:.4f}\")\n",
    "print(f\"最適化試行数: {len(study.trials)}\")\n",
    "\n",
    "print(\"\\n最適パラメータ:\")\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss', \n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param:20s}: {value}\")\n",
    "\n",
    "# 期待Kaggleスコア計算\n",
    "expected_kaggle = study.best_value * cv_kaggle_ratio\n",
    "improvement_from_exp004 = expected_kaggle - exp004_kaggle\n",
    "\n",
    "print(f\"\\n📈 改善分析:\")\n",
    "print(f\"最適化前CV: {best_result['cv_mean']:.4f}\")\n",
    "print(f\"最適化後CV: {study.best_value:.4f} ({study.best_value - best_result['cv_mean']:+.4f})\")\n",
    "print(f\"期待Kaggle: {expected_kaggle:.5f}\")\n",
    "print(f\"exp004から: {improvement_from_exp004:+.5f} ({improvement_from_exp004/exp004_kaggle*100:+.2f}%)\")\n",
    "\n",
    "if improvement_from_exp004 > 0:\n",
    "    print(\"✅ exp004からの改善が期待される！\")\n",
    "else:\n",
    "    print(\"⚠️ exp004からの改善は限定的\")\n",
    "\n",
    "print(\"\\nハイパーパラメータ最適化完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 最終モデル構築と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step 4: 最終モデル構築と評価 ===\")\n",
    "\n",
    "# 最終モデル訓練\n",
    "print(\"最終LightGBMモデル訓練開始...\")\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_cv_scores = []\n",
    "oof_predictions = np.zeros(len(X_optimal))\n",
    "test_predictions = np.zeros(len(X_test_optimal))\n",
    "models = []\n",
    "feature_importance_final = np.zeros(len(optimal_features))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_optimal, y), 1):\n",
    "    print(f\"Fold {fold}/5 訓練中...\")\n",
    "    \n",
    "    X_train, X_val = X_optimal.iloc[train_idx], X_optimal.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # 予測\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test_optimal)\n",
    "    \n",
    "    val_pred_binary = (val_pred >= 0.5).astype(int)\n",
    "    fold_score = accuracy_score(y_val, val_pred_binary)\n",
    "    final_cv_scores.append(fold_score)\n",
    "    \n",
    "    # OOF予測保存\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    test_predictions += test_pred / 5\n",
    "    \n",
    "    # モデルと重要度保存\n",
    "    models.append(model)\n",
    "    feature_importance_final += model.feature_importance()\n",
    "    \n",
    "    print(f\"Fold {fold} Accuracy: {fold_score:.4f} (Rounds: {model.best_iteration})\")\n",
    "\n",
    "# 最終結果計算\n",
    "final_cv_mean = np.mean(final_cv_scores)\n",
    "final_cv_std = np.std(final_cv_scores)\n",
    "oof_accuracy = accuracy_score(y, (oof_predictions >= 0.5).astype(int))\n",
    "feature_importance_final /= 5\n",
    "\n",
    "print(f\"\\n=== 最終モデル性能 ===\")\n",
    "print(f\"CV Accuracy: {final_cv_mean:.4f} ± {final_cv_std:.4f}\")\n",
    "print(f\"OOF Accuracy: {oof_accuracy:.4f}\")\n",
    "\n",
    "# exp004・exp005との比較\n",
    "print(f\"\\n=== 実験比較 ===\")\n",
    "print(f\"exp004 CV: 0.8462 ± 0.034\")\n",
    "print(f\"exp005 CV: 0.8507 ± 0.012 (過学習)\")\n",
    "print(f\"exp006 CV: {final_cv_mean:.4f} ± {final_cv_std:.4f}\")\n",
    "\n",
    "cv_improvement_004 = final_cv_mean - 0.8462\n",
    "print(f\"\\nexp004からのCV改善: {cv_improvement_004:+.4f}\")\n",
    "\n",
    "# 期待Kaggle性能\n",
    "final_expected_kaggle = final_cv_mean * cv_kaggle_ratio\n",
    "kaggle_improvement_004 = final_expected_kaggle - exp004_kaggle\n",
    "\n",
    "print(f\"\\n🎯 期待性能:\")\n",
    "print(f\"期待Kaggle: {final_expected_kaggle:.5f}\")\n",
    "print(f\"exp004から: {kaggle_improvement_004:+.5f} ({kaggle_improvement_004/exp004_kaggle*100:+.2f}%)\")\n",
    "\n",
    "if kaggle_improvement_004 > 0.005:\n",
    "    print(\"🎉 大幅改善が期待される！\")\n",
    "elif kaggle_improvement_004 > 0:\n",
    "    print(\"✅ 改善が期待される\")\n",
    "else:\n",
    "    print(\"⚠️ 改善は限定的\")\n",
    "\n",
    "# 最終特徴量重要度\n",
    "final_importance_df = pd.DataFrame({\n",
    "    'feature': optimal_features,\n",
    "    'importance': feature_importance_final\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n=== 最終特徴量重要度 ===\")\n",
    "for i, row in final_importance_df.iterrows():\n",
    "    print(f\"{final_importance_df.index.get_loc(i)+1:2d}. {row['feature']:20s}: {row['importance']:8.1f}\")\n",
    "\n",
    "# 重要度可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(final_importance_df)), final_importance_df['importance'])\n",
    "plt.yticks(range(len(final_importance_df)), final_importance_df['feature'])\n",
    "plt.xlabel('重要度')\n",
    "plt.title(f'exp006最終特徴量重要度 ({len(optimal_features)}特徴量)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ予測と提出ファイル生成\n",
    "test_predictions_binary = (test_predictions >= 0.5).astype(int)\n",
    "\n",
    "# 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_processed['PassengerId'],\n",
    "    'Survived': test_predictions_binary\n",
    "})\n",
    "\n",
    "# 結果保存\n",
    "import os\n",
    "os.makedirs('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp006', exist_ok=True)\n",
    "submission.to_csv('/Users/koki.ogai/Documents/ghq/github.com/oddgai/kaggle-projects/titanic/results/exp006/result.csv', index=False)\n",
    "\n",
    "print(f\"\\n=== 提出ファイル生成完了 ===\")\n",
    "print(f\"生存予測数: {test_predictions_binary.sum()}\")\n",
    "print(f\"死亡予測数: {len(test_predictions_binary) - test_predictions_binary.sum()}\")\n",
    "print(f\"予測生存率: {test_predictions_binary.mean():.3f}\")\n",
    "print(f\"\\n実際生存率: {y.mean():.3f}\")\n",
    "print(f\"予測vs実際: {test_predictions_binary.mean() - y.mean():+.3f}\")\n",
    "\n",
    "print(f\"\\n💾 提出ファイル: results/exp006/result.csv\")\n",
    "\n",
    "# 予測分布可視化\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(oof_predictions, bins=50, alpha=0.7, label='OOF Predictions')\n",
    "plt.axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
    "plt.xlabel('予測確率')\n",
    "plt.ylabel('頻度')\n",
    "plt.title('OOF予測分布')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_predictions, bins=50, alpha=0.7, label='Test Predictions')\n",
    "plt.axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
    "plt.xlabel('予測確率')\n",
    "plt.ylabel('頻度')\n",
    "plt.title('テスト予測分布')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp006 最終サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"                🎯 EXP006 最終結果サマリー\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 実験設定:\")\n",
    "print(f\"戦略: Less is More（シンプル化 + 精密最適化）\")\n",
    "print(f\"特徴量数: {len(optimal_features)} (exp004: 23, exp005: 45)\")\n",
    "print(f\"モデル: LightGBM単体\")\n",
    "print(f\"最適化: Optuna 100回試行\")\n",
    "\n",
    "print(f\"\\n📈 性能結果:\")\n",
    "print(f\"CV Accuracy: {final_cv_mean:.4f} ± {final_cv_std:.4f}\")\n",
    "print(f\"OOF Accuracy: {oof_accuracy:.4f}\")\n",
    "print(f\"期待Kaggle: {final_expected_kaggle:.5f}\")\n",
    "\n",
    "print(f\"\\n🎯 実験進捗比較:\")\n",
    "experiments = [\n",
    "    ('exp001', 0.77272, 0.8496),\n",
    "    ('exp004', 0.77990, 0.8462),\n",
    "    ('exp005', 0.76315, 0.8507),\n",
    "    ('exp006', '???', final_cv_mean)\n",
    "]\n",
    "\n",
    "for exp_name, kaggle, cv in experiments:\n",
    "    if exp_name == 'exp006':\n",
    "        print(f\"{exp_name}: Kaggle={kaggle}, CV={cv:.4f} ← 今回\")\n",
    "    else:\n",
    "        print(f\"{exp_name}: Kaggle={kaggle}, CV={cv:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 改善分析:\")\n",
    "print(f\"exp004からCV改善: {cv_improvement_004:+.4f} ({cv_improvement_004/0.8462*100:+.2f}%)\")\n",
    "print(f\"exp004から期待Kaggle改善: {kaggle_improvement_004:+.5f} ({kaggle_improvement_004/exp004_kaggle*100:+.2f}%)\")\n",
    "\n",
    "print(f\"\\n🔧 実装成果:\")\n",
    "print(f\"✅ 特徴量削減: 23→{len(optimal_features)}特徴量（{23-len(optimal_features)}個削減）\")\n",
    "print(f\"✅ ハイパーパラメータ最適化: Optuna 100回試行\")\n",
    "print(f\"✅ 過学習制御: CV標準偏差{final_cv_std:.4f}（安定性確保）\")\n",
    "print(f\"✅ 汎化性能重視: 期待乖離{abs(final_cv_mean - final_expected_kaggle):.4f}\")\n",
    "\n",
    "print(f\"\\n🎨 主要改善要因:\")\n",
    "print(f\"1. 戦略的特徴量削減（重要度ベース）\")\n",
    "print(f\"2. LightGBM精密最適化（正則化重視）\")\n",
    "print(f\"3. Less is More原理の実践\")\n",
    "print(f\"4. 汎化性能を重視した設計\")\n",
    "\n",
    "print(f\"\\n🏆 使用特徴量 Top 5:\")\n",
    "for i, row in final_importance_df.head(5).iterrows():\n",
    "    print(f\"{final_importance_df.index.get_loc(i)+1}. {row['feature']:20s}: {row['importance']:8.1f}\")\n",
    "\n",
    "print(f\"\\n💡 exp006の学び:\")\n",
    "if kaggle_improvement_004 > 0:\n",
    "    print(f\"🎉 シンプル化戦略が成功！\")\n",
    "    print(f\"📈 特徴量の質 > 量 の実証\")\n",
    "    print(f\"⚖️ 最適化と汎化のバランス\")\n",
    "else:\n",
    "    print(f\"📚 改善が限定的だが重要な学習\")\n",
    "    print(f\"🔍 Titanicデータの特性理解深化\")\n",
    "    print(f\"⚙️ 最適化手法の実践習得\")\n",
    "\n",
    "print(f\"\\n🎯 次の実験への示唆:\")\n",
    "if final_cv_mean > 0.850:\n",
    "    print(f\"・現在の手法で高い性能を達成\")\n",
    "    print(f\"・アンサンブル（異なる特徴量セット）で更なる向上\")\n",
    "else:\n",
    "    print(f\"・Neural Networkの導入検討\")\n",
    "    print(f\"・外部データ活用（歴史的事実）\")\n",
    "    print(f\"・より高度な特徴量エンジニアリング\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🚀 exp006完了！Kaggleでの結果をお楽しみに！\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}