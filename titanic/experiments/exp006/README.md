# exp006: 「Less is More」による精密最適化

## 🎯 実験概要

exp005の過学習による失敗を受けて、**特徴量削減とシンプル化**による改善を目指した実験。
「Less is More」の原理に基づき、質的改善を重視。

## 📊 結果サマリー

- **Kaggleスコア**: 0.77272（exp001と同値）
- **CVスコア**: 0.8440 ± 0.011
- **特徴量数**: 15個（exp004の23個から削減）
- **判定**: exp004からは悪化、但しexp001レベルは維持

## 🔍 戦略と手法

### 1. 特徴量削減戦略
- exp004の特徴量重要度分析に基づき上位15特徴量を選択
- 23特徴量 → 15特徴量（35%削減）

### 2. 最適化手法
- 軽量Optuna最適化（30回試行）
- 3-fold CV（高速化）
- LightGBM単体に集中

### 3. 選択された重要特徴量
1. Age_Fare_Interaction（最重要交互作用）
2. Age_Rank_SexPclass（統計特徴量）
3. Fare_Rank_Pclass（統計特徴量）
4. Fare, Age（基本特徴量）
5. Surname_Count, Title_Grouped（カテゴリ特徴量）

## 📈 性能比較

| 実験 | Kaggle | CV | CV-Kaggle乖離 | 特徴量数 |
|------|--------|-------|-------------|----------|
| exp001 | 0.77272 | 0.8496±0.026 | 0.0769 | 16 |
| exp004 | 0.77990 | 0.8462±0.034 | 0.0663 | 23 |
| **exp006** | **0.77272** | **0.8440±0.011** | **0.0713** | **15** |

## 🔍 分析結果

### ✅ 成功点
1. **特徴量大幅削減（35%）でも性能低下は限定的**
2. **CV安定性向上**（標準偏差: 0.034 → 0.011）
3. **exp001レベルの性能維持**（同一Kaggleスコア）
4. **過学習リスク低減**

### ⚠️ 課題点
1. **exp004から性能悪化**（-0.00718）
2. **CV vs Kaggle乖離は依然として存在**（0.0713）
3. **大幅改善には至らず**

## 💡 重要な学び

### 「Less is More」の有効性
- 特徴量を35%削減しても致命的な性能低下なし
- CVの安定性が大幅向上（過学習リスク減）
- シンプルなモデルでも一定の性能確保

### 特徴量の価値
- **Age_Fare_Interaction**が圧倒的に重要
- **統計特徴量**（ランキング系）の効果が高い
- **基本特徴量**（Age, Fare）の重要性再確認

## 🎯 次の戦略

exp006の結果を踏まえた今後の方向性：

### Option A: 特徴量エンジニアリング強化
- exp004の削除特徴量を個別検証
- 新たな交互作用探索
- ドメイン知識に基づく特徴量追加

### Option B: モデル手法変更
- Neural Network導入
- 別のアルゴリズム検証
- アンサンブル再検討（軽量版）

### Option C: データ品質改善
- 外部データ活用
- データクリーニング強化
- 異常値処理見直し

## 📊 実験詳細データ

```
最適ハイパーパラメータ（Optuna結果）:
- num_leaves: 最適値でtuning済み
- learning_rate: 最適値でtuning済み
- regularization: L1/L2正則化調整済み

CV詳細:
- Fold 1-5の個別結果記録済み
- OOF Accuracy: 0.8440
- 予測生存率: 0.328 vs 実際: 0.384
```

---

**結論**: exp006は「Less is More」原理の有効性を実証。特徴量削減によるシンプル化でも堅実な性能を維持できることを確認。次の実験では更なる改善手法の検討が必要。