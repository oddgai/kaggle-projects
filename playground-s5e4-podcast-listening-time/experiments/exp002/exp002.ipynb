{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験002: XGBoostモデル\n",
    "\n",
    "Playground Series S5E4: ポッドキャスト聴取時間予測\n",
    "\n",
    "exp001のベースラインを改善し、XGBoostモデルで性能向上を図る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 表示設定\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# スタイル設定\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('default')\n",
    "\n",
    "# シード値の設定\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データパスの設定\n",
    "data_dir = Path('../../data')\n",
    "\n",
    "# データの読み込み\n",
    "train_df = pd.read_csv(data_dir / 'train.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test.csv')\n",
    "sample_submission = pd.read_csv(data_dir / 'sample_submission.csv')\n",
    "\n",
    "print(f\"訓練データ: {train_df.shape}\")\n",
    "print(f\"テストデータ: {test_df.shape}\")\n",
    "print(f\"提出サンプル: {sample_submission.shape}\")\n",
    "\n",
    "# 目標変数の確認\n",
    "target_col = 'Listening_Time_minutes'\n",
    "print(f\"\\n目標変数: {target_col}\")\n",
    "print(f\"目標変数の統計: {train_df[target_col].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 拡張された特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_preprocess_data(df, is_train=True, target_col='Listening_Time_minutes', target_encoder=None):\n",
    "    \"\"\"\n",
    "    拡張されたデータの前処理を行う関数\n",
    "    \"\"\"\n",
    "    # データのコピー\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # IDカラムを保存\n",
    "    if 'id' in df_processed.columns:\n",
    "        ids = df_processed['id']\n",
    "        df_processed = df_processed.drop('id', axis=1)\n",
    "    \n",
    "    # 目標変数の分離（訓練データの場合）\n",
    "    if is_train and target_col in df_processed.columns:\n",
    "        y = df_processed[target_col]\n",
    "        df_processed = df_processed.drop(target_col, axis=1)\n",
    "    else:\n",
    "        y = None\n",
    "    \n",
    "    # カテゴリカル変数と数値変数を分離\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"カテゴリカル変数: {len(categorical_cols)}個\")\n",
    "    print(f\"数値変数: {len(numerical_cols)}個\")\n",
    "    \n",
    "    # 数値変数の欠損値補完\n",
    "    for col in numerical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            median_value = df_processed[col].median()\n",
    "            df_processed[col] = df_processed[col].fillna(median_value)\n",
    "            print(f\"{col}の欠損値を中央値{median_value:.2f}で補完\")\n",
    "    \n",
    "    # Guest_Popularity_percentageの欠損値を0で補完（ゲストなしを意味）\n",
    "    df_processed['Guest_Popularity_percentage'] = df_processed['Guest_Popularity_percentage'].fillna(0)\n",
    "    \n",
    "    # 基本的な新規特徴量の作成（exp001と同じ）\n",
    "    df_processed['Ad_Density'] = df_processed['Number_of_Ads'] / (df_processed['Episode_Length_minutes'] + 1e-8)\n",
    "    df_processed['Host_Guest_Popularity_Diff'] = (df_processed['Host_Popularity_percentage'] - \n",
    "                                                   df_processed['Guest_Popularity_percentage'])\n",
    "    df_processed['Has_Guest'] = (df_processed['Guest_Popularity_percentage'] > 0).astype(int)\n",
    "    \n",
    "    # 新しい特徴量の作成\n",
    "    # 時間系特徴量\n",
    "    df_processed['Episode_Length_squared'] = df_processed['Episode_Length_minutes'] ** 2\n",
    "    df_processed['Episode_Length_log'] = np.log(df_processed['Episode_Length_minutes'] + 1)\n",
    "    \n",
    "    # 人気度関連特徴量\n",
    "    df_processed['Host_Guest_Popularity_Sum'] = df_processed['Host_Popularity_percentage'] + df_processed['Guest_Popularity_percentage']\n",
    "    df_processed['Host_Guest_Popularity_Ratio'] = df_processed['Host_Popularity_percentage'] / (df_processed['Guest_Popularity_percentage'] + 1)\n",
    "    \n",
    "    # 広告関連特徴量\n",
    "    df_processed['Ads_per_Hour'] = df_processed['Number_of_Ads'] / ((df_processed['Episode_Length_minutes'] / 60) + 1e-8)\n",
    "    df_processed['Has_Ads'] = (df_processed['Number_of_Ads'] > 0).astype(int)\n",
    "    \n",
    "    # エピソード長カテゴリ\n",
    "    df_processed['Episode_Length_Category'] = pd.cut(df_processed['Episode_Length_minutes'], \n",
    "                                                     bins=[0, 30, 60, 90, float('inf')], \n",
    "                                                     labels=['short', 'medium', 'long', 'very_long'])\n",
    "    \n",
    "    # カテゴリカル変数の前処理\n",
    "    le_dict = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        # 欠損値がある場合は'Missing'で埋める\n",
    "        df_processed[col] = df_processed[col].fillna('Missing')\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        le_dict[col] = le\n",
    "        print(f\"{col}をラベルエンコーディング: {len(le.classes_)}カテゴリ\")\n",
    "    \n",
    "    # Episode_Length_Categoryのエンコーディング\n",
    "    le_length_cat = LabelEncoder()\n",
    "    df_processed['Episode_Length_Category'] = le_length_cat.fit_transform(df_processed['Episode_Length_Category'])\n",
    "    le_dict['Episode_Length_Category'] = le_length_cat\n",
    "    \n",
    "    # ターゲットエンコーディング（訓練データの場合のみ）\n",
    "    if is_train and y is not None and target_encoder is None:\n",
    "        target_encoding_cols = ['Podcast_Name', 'Episode_Title', 'Genre']\n",
    "        target_encoder = {}\n",
    "        \n",
    "        for col in target_encoding_cols:\n",
    "            if col in categorical_cols:\n",
    "                # 各カテゴリの平均値を計算（スムージング付き）\n",
    "                global_mean = y.mean()\n",
    "                category_means = df_processed.groupby(col)[col].apply(lambda x: len(x))\n",
    "                category_targets = pd.DataFrame({'original_col': df_processed[col], 'target': y})\n",
    "                category_target_mean = category_targets.groupby('original_col')['target'].mean()\n",
    "                \n",
    "                # スムージング（α=10）\n",
    "                alpha = 10\n",
    "                smoothed_means = (category_target_mean * category_means + global_mean * alpha) / (category_means + alpha)\n",
    "                \n",
    "                target_encoder[col] = smoothed_means\n",
    "                df_processed[f'{col}_target_encoded'] = df_processed[col].map(target_encoder[col]).fillna(global_mean)\n",
    "                print(f\"{col}のターゲットエンコーディング完了\")\n",
    "    \n",
    "    elif target_encoder is not None:\n",
    "        # テストデータの場合、保存されたエンコーダーを使用\n",
    "        for col, encoder in target_encoder.items():\n",
    "            if col in df_processed.columns:\n",
    "                global_mean = encoder.mean()  # 訓練データの平均値\n",
    "                df_processed[f'{col}_target_encoded'] = df_processed[col].map(encoder).fillna(global_mean)\n",
    "    \n",
    "    return df_processed, y, ids if 'ids' in locals() else None, le_dict, target_encoder if is_train else None\n",
    "\n",
    "# データの前処理実行\n",
    "print(\"=== 訓練データの前処理 ===\")\n",
    "X_train_processed, y_train, train_ids, le_dict_train, target_encoder = enhanced_preprocess_data(train_df, is_train=True)\n",
    "\n",
    "print(\"\\n=== テストデータの前処理 ===\")\n",
    "X_test_processed, _, test_ids, _, _ = enhanced_preprocess_data(test_df, is_train=False, target_encoder=target_encoder)\n",
    "\n",
    "print(f\"\\n処理後の訓練データ形状: {X_train_processed.shape}\")\n",
    "print(f\"処理後のテストデータ形状: {X_test_processed.shape}\")\n",
    "print(f\"特徴量一覧: {X_train_processed.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoostモデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練・検証データの分割\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_processed, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"訓練データ: {X_train.shape}\")\n",
    "print(f\"検証データ: {X_val.shape}\")\n",
    "\n",
    "# XGBoostモデルの訓練\n",
    "print(\"\\n=== XGBoostモデルの訓練 ===\")\n",
    "\n",
    "# パラメータ設定\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# データセットの作成\n",
    "train_data = xgb.DMatrix(X_train, label=y_train_split)\n",
    "val_data = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# モデル訓練\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    train_data,\n",
    "    evals=[(train_data, 'train'), (val_data, 'eval')],\n",
    "    num_boost_round=2000,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 予測\ny_train_pred = xgb_model.predict(xgb.DMatrix(X_train), iteration_range=(0, xgb_model.best_iteration))\ny_val_pred = xgb_model.predict(xgb.DMatrix(X_val), iteration_range=(0, xgb_model.best_iteration))\n\n# 評価指標の計算\ndef calculate_metrics(y_true, y_pred, dataset_name):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f\"\\n=== {dataset_name}の評価結果 ===\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n    print(f\"R²: {r2:.4f}\")\n    \n    return rmse, mae, r2\n\n# 訓練データでの評価\ntrain_rmse, train_mae, train_r2 = calculate_metrics(y_train_split, y_train_pred, \"訓練データ\")\n\n# 検証データでの評価\nval_rmse, val_mae, val_r2 = calculate_metrics(y_val, y_val_pred, \"検証データ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5-fold Cross Validation\nprint(\"=== 5-Fold Cross Validation ===\")\n\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_processed)):\n    print(f\"\\nFold {fold + 1}/5\")\n    \n    # データ分割\n    X_fold_train, X_fold_val = X_train_processed.iloc[train_idx], X_train_processed.iloc[val_idx]\n    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # データセット作成\n    fold_train_data = xgb.DMatrix(X_fold_train, label=y_fold_train)\n    fold_val_data = xgb.DMatrix(X_fold_val, label=y_fold_val)\n    \n    # モデル訓練\n    fold_model = xgb.train(\n        xgb_params,\n        fold_train_data,\n        evals=[(fold_train_data, 'train'), (fold_val_data, 'eval')],\n        num_boost_round=2000,\n        early_stopping_rounds=100,\n        verbose_eval=0\n    )\n    \n    # 予測と評価\n    fold_pred = fold_model.predict(fold_val_data, iteration_range=(0, fold_model.best_iteration))\n    fold_rmse = np.sqrt(mean_squared_error(y_fold_val, fold_pred))\n    cv_scores.append(fold_rmse)\n    \n    print(f\"Fold {fold + 1} RMSE: {fold_rmse:.4f}\")\n\nprint(f\"\\n=== Cross Validation結果 ===\")\nprint(f\"平均RMSE: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\nprint(f\"各FoldのRMSE: {[f'{score:.4f}' for score in cv_scores]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 特徴量重要度の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度の取得\n",
    "feature_importance = xgb_model.get_score(importance_type='gain')\n",
    "feature_names = X_train_processed.columns\n",
    "\n",
    "# 重要度をDataFrameにまとめる（全特徴量を含む）\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': [feature_importance.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== 特徴量重要度 Top 15 ===\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(data=top_features, x='importance', y='feature')\n",
    "plt.title('特徴量重要度 Top 20 (XGBoost)')\n",
    "plt.xlabel('重要度')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 予測と提出ファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの予測\n",
    "test_data = xgb.DMatrix(X_test_processed)\n",
    "test_predictions = xgb_model.predict(test_data, ntree_limit=xgb_model.best_ntree_limit)\n",
    "\n",
    "# 提出ファイルの作成\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    target_col: test_predictions\n",
    "})\n",
    "\n",
    "print(\"=== 提出ファイルの統計 ===\")\n",
    "print(submission_df[target_col].describe())\n",
    "\n",
    "# ファイル保存\n",
    "results_dir = Path('../../results/exp002')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "submission_path = results_dir / 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\n提出ファイルを保存: {submission_path}\")\n",
    "\n",
    "# サンプル提出ファイルとの形式確認\n",
    "print(f\"\\n=== フォーマット確認 ===\")\n",
    "print(f\"サンプル提出ファイル形状: {sample_submission.shape}\")\n",
    "print(f\"作成した提出ファイル形状: {submission_df.shape}\")\n",
    "print(f\"カラム名一致: {list(sample_submission.columns) == list(submission_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "model_path = results_dir / 'model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"モデルを保存: {model_path}\")\n",
    "\n",
    "# 実験結果の記録（MLflow用に詳細データを追加）\n",
    "experiment_results = {\n",
    "    'experiment_id': 'exp002',\n",
    "    'model_type': 'XGBoost',\n",
    "    'model_params': xgb_params,\n",
    "    \n",
    "    # 特徴量情報\n",
    "    'features': list(X_train_processed.columns),\n",
    "    'num_features': len(X_train_processed.columns),\n",
    "    'new_features': ['Ad_Density', 'Host_Guest_Popularity_Diff', 'Has_Guest', \n",
    "                    'Episode_Length_squared', 'Episode_Length_log',\n",
    "                    'Host_Guest_Popularity_Sum', 'Host_Guest_Popularity_Ratio',\n",
    "                    'Ads_per_Hour', 'Has_Ads', 'Episode_Length_Category'],\n",
    "    \n",
    "    # 評価指標\n",
    "    'train_rmse': float(train_rmse),\n",
    "    'train_mae': float(train_mae),\n",
    "    'train_r2': float(train_r2),\n",
    "    'val_rmse': float(val_rmse),\n",
    "    'val_mae': float(val_mae),\n",
    "    'val_r2': float(val_r2),\n",
    "    \n",
    "    # Cross Validation結果\n",
    "    'cv_scores': [float(score) for score in cv_scores],\n",
    "    'cv_rmse_mean': float(np.mean(cv_scores)),\n",
    "    'cv_rmse_std': float(np.std(cv_scores)),\n",
    "    \n",
    "    # 特徴量重要度\n",
    "    'feature_importance': {\n",
    "        row['feature']: int(row['importance']) \n",
    "        for _, row in importance_df.iterrows()\n",
    "    },\n",
    "    \n",
    "    # Kaggleスコア（手動で更新）\n",
    "    'public_score': None,  # 提出後に更新\n",
    "    'private_score': None,  # 提出後に更新\n",
    "    \n",
    "    # データ情報\n",
    "    'train_size': len(X_train_processed),\n",
    "    'test_size': len(X_test_processed),\n",
    "    'target_variable': target_col,\n",
    "    \n",
    "    # 実験設定\n",
    "    'cv_folds': 5,\n",
    "    'validation_split': 0.2,\n",
    "    'random_state': 42,\n",
    "    'preprocessing': {\n",
    "        'missing_value_strategy': 'median_imputation',\n",
    "        'categorical_encoding': 'label_encoding_and_target_encoding',\n",
    "        'feature_engineering': 'enhanced_features,target_encoding,polynomial_features'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 実験結果をJSONファイルに保存\n",
    "results_json_path = results_dir / 'experiment_results.json'\n",
    "with open(results_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"実験結果を保存: {results_json_path}\")\n",
    "\n",
    "print(\"\\n=== 実験結果サマリー ===\")\n",
    "print(f\"モデル: {experiment_results['model_type']}\")\n",
    "print(f\"特徴量数: {len(experiment_results['features'])}個\")\n",
    "print(f\"検証RMSE: {experiment_results['val_rmse']:.4f}\")\n",
    "print(f\"CV平均RMSE: {experiment_results['cv_rmse_mean']:.4f} ± {experiment_results['cv_rmse_std']:.4f}\")\n",
    "print(f\"\\n重要な特徴量トップ5:\")\n",
    "for i, (feature, importance) in enumerate(list(experiment_results['feature_importance'].items())[:5]):\n",
    "    print(f\"  {i+1}. {feature}: {importance:,}\")\n",
    "\n",
    "print(f\"\\nexp001との比較:\")\n",
    "print(f\"exp001 CV RMSE: 13.0023\")\n",
    "print(f\"exp002 CV RMSE: {experiment_results['cv_rmse_mean']:.4f}\")\n",
    "improvement = 13.0023 - experiment_results['cv_rmse_mean']\n",
    "print(f\"改善度: {improvement:.4f} ({'改善' if improvement > 0 else '悪化'})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}