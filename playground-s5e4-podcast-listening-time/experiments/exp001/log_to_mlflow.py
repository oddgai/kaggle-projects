#!/usr/bin/env python3
"""
MLflow logging script for exp001: LightGBM Baseline Model
Playground Series S5E4: Podcast Listening Time Prediction
"""

import json
import pickle
import re
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import mlflow
import mlflow.lightgbm
import mlflow.sklearn
import numpy as np
import pandas as pd


def markdown_to_html(markdown_text):
    """
    ç°¡å˜ãªMarkdown -> HTMLå¤‰æ›
    """
    html = markdown_text

    # ãƒ˜ãƒƒãƒ€ãƒ¼å¤‰æ›
    html = re.sub(r"^# (.*)", r"<h1>\1</h1>", html, flags=re.MULTILINE)
    html = re.sub(r"^## (.*)", r"<h2>\1</h2>", html, flags=re.MULTILINE)
    html = re.sub(r"^### (.*)", r"<h3>\1</h3>", html, flags=re.MULTILINE)
    html = re.sub(r"^#### (.*)", r"<h4>\1</h4>", html, flags=re.MULTILINE)

    # ãƒªã‚¹ãƒˆå¤‰æ›
    html = re.sub(r"^- (.*)", r"<li>\1</li>", html, flags=re.MULTILINE)

    # ç•ªå·ä»˜ããƒªã‚¹ãƒˆå¤‰æ›
    html = re.sub(r"^(\d+)\. (.*)", r"<li>\2</li>", html, flags=re.MULTILINE)

    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®å€‹åˆ¥åŒ–ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ç‰¹å¾´é‡ã‚’å€‹åˆ¥ã®liã«åˆ†è§£ï¼‰
    def expand_feature_list(match):
        features = match.group(2)
        feature_list = [f.strip() for f in features.split(",")]
        li_items = "".join([f"<li>{feature}</li>" for feature in feature_list])
        return li_items

    html = re.sub(
        r"<li>(ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°|æ•°å€¤å¤‰æ•°): ([^<]+)</li>", expand_feature_list, html
    )

    # ãƒ†ãƒ¼ãƒ–ãƒ«å‡¦ç†ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    lines = html.split("\n")
    in_table = False
    result_lines = []

    for line in lines:
        if "|" in line and line.strip().startswith("|"):
            if not in_table:
                result_lines.append(
                    '<table border="1" style="border-collapse: collapse;">'
                )
                in_table = True

            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã®å‡¦ç†
            cells = [
                cell.strip() for cell in line.split("|")[1:-1]
            ]  # æœ€åˆã¨æœ€å¾Œã®ç©ºè¦ç´ ã‚’é™¤å¤–
            if all(
                cell in ["---", "------", "--------", "----------", "------------"]
                or "-" in cell
                for cell in cells
            ):
                continue  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—

            row_html = (
                "<tr>" + "".join([f"<td>{cell}</td>" for cell in cells]) + "</tr>"
            )
            result_lines.append(row_html)
        else:
            if in_table:
                result_lines.append("</table>")
                in_table = False
            result_lines.append(line)

    if in_table:
        result_lines.append("</table>")

    html = "\n".join(result_lines)

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®JSONãƒ†ãƒ¼ãƒ–ãƒ«å¤‰æ›
    def convert_hyperparams_to_table(match):
        code_content = match.group(1)
        if "lgb_params" in code_content and "{" in code_content:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¤‰æ›
            params_html = (
                '<table border="1" style="border-collapse: collapse; margin: 10px 0;">'
            )
            params_html += '<tr><th style="padding: 8px; background-color: #f0f0f0;">Parameter</th>'
            params_html += (
                '<th style="padding: 8px; background-color: #f0f0f0;">Value</th></tr>'
            )

            # ç°¡æ˜“çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾ã§ï¼‰
            param_lines = [
                ("objective", "'regression'"),
                ("metric", "'rmse'"),
                ("boosting_type", "'gbdt'"),
                ("num_leaves", "31"),
                ("learning_rate", "0.05"),
                ("feature_fraction", "0.9"),
                ("bagging_fraction", "0.8"),
                ("bagging_freq", "5"),
                ("verbose", "-1"),
                ("random_state", "42"),
            ]

            for param, value in param_lines:
                params_html += f'<tr><td style="padding: 8px;">{param}</td>'
                params_html += f'<td style="padding: 8px;">{value}</td></tr>'

            params_html += "</table>"
            return params_html
        else:
            return f"<pre><code>{code_content}</code></pre>"

    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç‰¹åˆ¥æ‰±ã„ï¼‰
    html = re.sub(r"```(.*?)```", convert_hyperparams_to_table, html, flags=re.DOTALL)
    html = re.sub(r"`([^`]*)`", r"<code>\1</code>", html)

    # å¼·èª¿
    html = re.sub(r"\*\*(.*?)\*\*", r"<strong>\1</strong>", html)
    html = re.sub(r"\*(.*?)\*", r"<em>\1</em>", html)

    # æ”¹è¡Œå‡¦ç†ï¼ˆæœ€å°é™ï¼‰
    html = html.replace("\n\n", "\n")  # æ®µè½åŒºåˆ‡ã‚Šã‚’å˜ä¸€æ”¹è¡Œã«
    html = re.sub(r"\n+", " ", html)  # æ®‹ã‚Šã®æ”¹è¡Œã‚’ç©ºç™½ã«

    # ãƒªã‚¹ãƒˆé …ç›®ã‚’ulã§å›²ã‚€
    html = re.sub(r"(<li>.*?</li>)", r"<ul>\1</ul>", html, flags=re.DOTALL)
    html = re.sub(r"</ul> <ul>", "</ul><ul>", html)

    return f"<html><body>{html}</body></html>"


def log_experiment_to_mlflow():
    """
    exp001ã®å®Ÿé¨“çµæœã‚’MLflowã«è¨˜éŒ²ã™ã‚‹
    """

    # Databricks MLflowã‚’ä½¿ç”¨
    mlflow.set_tracking_uri("databricks")

    # å®Ÿé¨“åã‚’è¨­å®š
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    experiment_name = (
        "/Shared/data_science/z_ogai/playground-s5e4-podcast-listening-time"
    )

    try:
        mlflow.set_experiment(experiment_name)
        print(f"âœ… Databrickså®Ÿé¨“ã‚’è¨­å®š: {experiment_name}")
    except Exception as e:
        print(f"âš ï¸  Databricksæ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ãƒ­ãƒ¼ã‚«ãƒ«MLflowã«åˆ‡ã‚Šæ›¿ãˆ: {e}")
        mlflow.set_tracking_uri("file:./mlruns")
        experiment_name = (
            "/Shared/data_science/z_ogai/playground-s5e4-podcast-listening-time"
        )
        mlflow.set_experiment(experiment_name)

    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    results_dir = Path("../../results/exp001")
    
    # å®Ÿé¨“çµæœã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    results_json_path = results_dir / 'experiment_results.json'
    if not results_json_path.exists():
        raise FileNotFoundError(f"å®Ÿé¨“çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_json_path}")
    
    with open(results_json_path, 'r', encoding='utf-8') as f:
        exp_results = json.load(f)
    
    print(f"âœ… å®Ÿé¨“çµæœã‚’èª­ã¿è¾¼ã¿: {results_json_path}")

    # Runè¨­å®š
    run_name = f"exp001_{timestamp}"
    run_description = "LightGBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã€‚åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆAd_Density, Has_Guest, Host_Guest_Popularity_Diffï¼‰ã‚’å®Ÿè£…ã—ãŸåˆå›å®Ÿé¨“ã€‚exp000ï¼ˆEDAï¼‰ã‹ã‚‰æ´¾ç”Ÿã€‚"

    with mlflow.start_run(run_name=run_name, description=run_description) as run:
        # ===================
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
        # ===================

        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        model_params = exp_results["model_params"]

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
        for param_name, param_value in model_params.items():
            mlflow.log_param(f"lgb_{param_name}", param_value)

        # å®Ÿé¨“è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        mlflow.log_param("model_type", exp_results["model_type"])
        mlflow.log_param("experiment_id", exp_results["experiment_id"])
        mlflow.log_param("task_type", "regression")
        mlflow.log_param("cv_folds", exp_results["cv_folds"])
        mlflow.log_param("validation_split", exp_results["validation_split"])
        mlflow.log_param("random_state", exp_results["random_state"])

        # ç‰¹å¾´é‡æƒ…å ±ï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        features = exp_results["features"]
        new_features = exp_results["new_features"]
        
        mlflow.log_param("num_features", exp_results["num_features"])
        mlflow.log_param("feature_engineering", exp_results["preprocessing"]["feature_engineering"])
        mlflow.log_param("new_features", ",".join(new_features))
        mlflow.log_param("num_new_features", len(new_features))

        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        mlflow.log_param("train_size", exp_results["train_size"])
        mlflow.log_param("test_size", exp_results["test_size"])
        mlflow.log_param("target_variable", exp_results["target_variable"])

        # ===================
        # è©•ä¾¡æŒ‡æ¨™ã®è¨˜éŒ²
        # ===================

        # Cross Validationçµæœï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        cv_scores = exp_results["cv_scores"]
        cv_mean = exp_results["cv_rmse_mean"]
        cv_std = exp_results["cv_rmse_std"]

        mlflow.log_metric("cv_rmse_mean", cv_mean)
        mlflow.log_metric("cv_rmse_std", cv_std)
        for i, score in enumerate(cv_scores, 1):
            mlflow.log_metric(f"cv_rmse_fold{i}", score)

        # è¨“ç·´ãƒ»æ¤œè¨¼çµæœï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        mlflow.log_metric("train_rmse", exp_results["train_rmse"])
        mlflow.log_metric("val_rmse", exp_results["val_rmse"])
        mlflow.log_metric("train_mae", exp_results["train_mae"])
        mlflow.log_metric("val_mae", exp_results["val_mae"])
        mlflow.log_metric("val_r2", exp_results["val_r2"])

        # Kaggleæå‡ºçµæœï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        public_score = exp_results["public_score"]
        private_score = exp_results["private_score"]
        mlflow.log_metric("public_score", public_score)
        mlflow.log_metric("private_score", private_score)

        # ã‚¹ã‚³ã‚¢ä¸€è²«æ€§ã®æŒ‡æ¨™
        cv_public_diff = abs(cv_mean - public_score)
        cv_private_diff = abs(cv_mean - private_score)
        mlflow.log_metric("cv_public_diff", cv_public_diff)
        mlflow.log_metric("cv_private_diff", cv_private_diff)

        # ===================
        # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®è¨˜éŒ²
        # ===================

        # ç‰¹å¾´é‡é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        feature_importance_data = exp_results["feature_importance"]

        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’CSVã¨ã—ã¦ä¿å­˜
        importance_df = pd.DataFrame(
            [
                {"feature": feature, "importance": importance}
                for feature, importance in feature_importance_data.items()
            ]
        )
        importance_file = "feature_importance.csv"
        importance_df.to_csv(importance_file, index=False)
        mlflow.log_artifact(importance_file)

        # ç‰¹å¾´é‡é‡è¦åº¦ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        plt.figure(figsize=(12, 8))
        importance_sorted = importance_df.sort_values("importance", ascending=True)

        # æ¨ªæ£’ã‚°ãƒ©ãƒ•
        bars = plt.barh(range(len(importance_sorted)), importance_sorted["importance"])
        plt.yticks(range(len(importance_sorted)), importance_sorted["feature"])
        plt.xlabel("Feature Importance")
        plt.title("Feature Importance - LightGBM Model (exp001)")
        plt.grid(axis="x", alpha=0.3)

        # ã‚«ãƒ©ãƒ¼ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        colors = plt.cm.viridis(np.linspace(0, 1, len(bars)))
        for bar, color in zip(bars, colors):
            bar.set_color(color)

        plt.tight_layout()

        # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
        feature_importance_plot = "feature_importance_plot.png"
        plt.savefig(feature_importance_plot, dpi=80, bbox_inches="tight")
        mlflow.log_artifact(feature_importance_plot)
        plt.close()

        # å®Ÿé¨“ã‚µãƒãƒªãƒ¼ã®ä½œæˆï¼ˆJSONãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«æ›´æ–°ï¼‰
        experiment_summary = {
            "experiment_id": exp_results["experiment_id"],
            "model_type": exp_results["model_type"],
            "objective": "Baseline model for podcast listening time prediction",
            "date": datetime.now().isoformat(),
            "data_info": {
                "train_samples": exp_results["train_size"],
                "test_samples": exp_results["test_size"],
                "features": exp_results["num_features"],
                "target": exp_results["target_variable"],
            },
            "preprocessing": exp_results["preprocessing"],
            "results": {
                "cv_rmse": f"{cv_mean:.4f} Â± {cv_std:.4f}",
                "public_score": public_score,
                "private_score": private_score,
                "validation_r2": exp_results["val_r2"],
            },
            "key_findings": [
                "Episode_Length_minutes is the most important feature",
                "Ad_Density (new feature) ranks 2nd in importance",
                "CV score closely matches public/private scores",
                "No overfitting detected",
                "Stable model performance across folds",
            ],
            "next_steps": [
                "Target encoding for categorical variables",
                "Feature interactions (genre x popularity)",
                "Hyperparameter optimization",
                "Ensemble with XGBoost",
            ],
        }

        # ã‚µãƒãƒªãƒ¼ã‚’JSONã¨ã—ã¦ä¿å­˜
        summary_file = "experiment_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(experiment_summary, f, indent=2, ensure_ascii=False)
        mlflow.log_artifact(summary_file)

        # README.mdã‚’HTMLã«å¤‰æ›ã—ã¦ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦è¿½åŠ 
        readme_path = Path("README.md")
        if readme_path.exists():
            with open(readme_path, "r", encoding="utf-8") as f:
                readme_content = f.read()

            # Markdownã‚’HTMLã«å¤‰æ›
            readme_html = markdown_to_html(readme_content)

            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            readme_html_path = "README.html"
            with open(readme_html_path, "w", encoding="utf-8") as f:
                f.write(readme_html)

            mlflow.log_artifact(readme_html_path, "documentation")

        # ===================
        # ãƒ¢ãƒ‡ãƒ«ã®è¨˜éŒ²
        # ===================

        # LightGBMãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¨˜éŒ²
        model_path = results_dir / "model.pkl"
        if model_path.exists():
            # pickleãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦è¨˜éŒ²
            mlflow.log_artifact(str(model_path), "model")

            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§MLflowãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è¨˜éŒ²
            with open(model_path, "rb") as f:
                model = pickle.load(f)

            # ã‚·ã‚°ãƒãƒãƒ£æƒ…å ±ï¼ˆå…¥åŠ›ç‰¹å¾´é‡ã®æƒ…å ±ï¼‰
            from mlflow.models.signature import infer_signature

            # ãƒ€ãƒŸãƒ¼ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã§ã‚·ã‚°ãƒãƒãƒ£ã‚’ä½œæˆ
            dummy_input = pd.DataFrame({feature: [0] * 5 for feature in features})
            dummy_output = np.array([45.0] * 5)  # å¹³å‡çš„ãªè´å–æ™‚é–“

            signature = infer_signature(dummy_input, dummy_output)

            # LightGBMãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è¨˜éŒ²
            mlflow.lightgbm.log_model(
                model,
                "lightgbm_model",
                signature=signature,
                input_example=dummy_input.iloc[0:1],
            )

        print(f"âœ… MLflow experiment logged successfully!")
        print(f"Run ID: {run.info.run_id}")
        print(f"Experiment ID: {run.info.experiment_id}")
        print(f"Artifact URI: {run.info.artifact_uri}")

        return run.info.run_id


if __name__ == "__main__":
    run_id = log_experiment_to_mlflow()
    print(f"\\nğŸ¯ å®Ÿé¨“ãŒMLflowã«è¨˜éŒ²ã•ã‚Œã¾ã—ãŸ")
    print(f"Run ID: {run_id}")
